# Chapter 9: Synthesis - Score-Based Diffusion and Beyond

> **ì±… í˜ì´ì§€**: 266+
> **í•µì‹¬ ì£¼ì œ**: Score-Based Diffusion, Bridge Diffusion, VAE/GANì˜ Diffusion í•´ì„, Phase Transitions, RL, GFlowNets
> **KAIST Challenge ì—°ê²°**: Challenge 19 (Score Matching), Challenge 20 (Diffusion Models)

---

## ğŸ“š ëª©ì°¨

1. [Score-Based Diffusion Models](#1-score-based-diffusion-models)
2. [Bridge Diffusionê³¼ SchrÃ¶dinger Bridge](#2-bridge-diffusionê³¼-schrÃ¶dinger-bridge)
3. [VAEì™€ GANì˜ Diffusion í•´ì„](#3-vaeì™€-ganì˜-diffusion-í•´ì„)
4. [Dynamic Phase Transitions](#4-dynamic-phase-transitions)
5. [Stochastic Optimal Controlê³¼ RL](#5-stochastic-optimal-controlê³¼-rl)
6. [Generative Flow Networks (GFlowNets)](#6-generative-flow-networks-gflownets)
7. [Notebooks ê°€ì´ë“œ](#7-notebooks-ê°€ì´ë“œ)
8. [ì „ì²´ ì±… í†µí•© ìš”ì•½](#8-ì „ì²´-ì±…-í†µí•©-ìš”ì•½)

---

## 1. Score-Based Diffusion Models

### í•µì‹¬ ì•„ì´ë””ì–´

> **ì±… ì›ë¬¸ (p.266):**
> "Score-Based Diffusions currently represent the state of the art... a forward-time process that incrementally corrupts ground-truth samples by adding noise, and a reverse-time process that reconstructs data by gradually removing noise."

```
Forward Process (ë…¸ì´ì¦ˆ ì¶”ê°€):
    xâ‚€ (ë°ì´í„°) â†’ xâ‚ â†’ xâ‚‚ â†’ ... â†’ x_T â‰ˆ N(0, I)

Reverse Process (ë””ë…¸ì´ì§•):
    x_T (ë…¸ì´ì¦ˆ) â†’ ... â†’ xâ‚ â†’ xâ‚€ (ìƒì„±ëœ ë°ì´í„°)
```

### Forward SDE

$$dx_t = f(x_t, t) dt + g(t) dw_t$$

```
f(x, t): drift (ë³´í†µ ì„ í˜•)
g(t): diffusion coefficient
w_t: Brownian motion

ì˜ˆ: Variance Preserving (VP) SDE
    dx = -Â½ Î²(t) x dt + âˆšÎ²(t) dw
```

### Reverse SDE

$$dy_t = [f(y_t, t) - g(t)^2 \nabla \log p(y_t, t)] dt + g(t) d\bar{w}_t$$

**í•µì‹¬**: Score function $\nabla \log p(x, t)$ê°€ í•„ìš”!

### Score Matching

```
Score: s(x, t) = âˆ‡_x log p(x, t)

"í™•ë¥ ì´ ì¦ê°€í•˜ëŠ” ë°©í–¥"

í•™ìŠµ ëª©í‘œ:
    min_Î¸ E[||s_Î¸(x_t, t) - âˆ‡ log p(x_t|x_0)||Â²]

Denoising Score Matching:
    s(x_t, t) â‰ˆ (x_0 - x_t) / Ïƒ_tÂ²
    "ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ ë°©í–¥ì˜ ì—­ë°©í–¥"
```

### Anderson's Theorem

> **ì±… ì›ë¬¸ (p.268):**
> "By imposing that both dynamics describe the same time-marginal p(x, t), one sees that the two Fokkerâ€“Planck equations are consistent."

```
Forwardì™€ Reverseì˜ marginal ë¶„í¬ê°€ ì¼ì¹˜!
    p_forward(x, t) = p_reverse(x, t)

ì´ê²ƒì´ Diffusionì´ ì •í™•í•œ ìƒì„± ëª¨ë¸ì¸ ì´ìœ 
```

### í•™ìŠµ íŒŒì´í”„ë¼ì¸

```python
# Training
for x_0 in dataset:
    t = random_time()
    noise = random_gaussian()
    x_t = forward(x_0, t, noise)

    predicted_noise = score_net(x_t, t)
    loss = ||predicted_noise - noise||Â²

# Generation
x_T = random_gaussian()
for t in reversed(times):
    x_{t-1} = reverse_step(x_t, score_net, t)
return x_0
```

---

## 2. Bridge Diffusionê³¼ SchrÃ¶dinger Bridge

### SchrÃ¶dinger Bridge ë¬¸ì œ

> **ì±… ì›ë¬¸ (p.272):**
> "Can we keep T finite while also making statistics of x(T) fixed?"

```
Standard Diffusion:
    x(0) ~ p_data
    x(T) ~ N(0, I)  (T â†’ âˆì—ì„œ)

Bridge Diffusion:
    x(0) ~ p_data
    x(1) = x_target (ê³ ì •!)
    T = 1ë¡œ ìœ í•œ
```

### Doob's h-transform

$$dx(t) = [f(t; x) + G \nabla_x \log p(x(1)|x(t))] dt + \sqrt{G} dw_t$$

```
ì¶”ê°€ëœ drift: G âˆ‡ log p(x(1)|x(t))

"ëª©í‘œ x(1)ì„ í–¥í•´ ìœ ë„"
â†’ ëª¨ë“  ê²½ë¡œê°€ x(1)ì—ì„œ ëë‚¨!
```

### Optimal Transport ì—°ê²°

```
SchrÃ¶dinger Bridge â†” Entropic Optimal Transport

Îµ â†’ 0:
    í™•ë¥  ê²½ë¡œ â†’ ê²°ì •ë¡ ì  OT map

GANê³¼ì˜ ì—°ê²°:
    GAN = Zero-noise SchrÃ¶dinger Bridge
    ë‹¨ì¼ ìŠ¤í…, ê²°ì •ë¡ ì  ë³€í™˜
```

---

## 3. VAEì™€ GANì˜ Diffusion í•´ì„

### í†µí•©ëœ ê´€ì 

> **ì±… ì›ë¬¸ (p.272):**
> "Many pre-diffusion generative models can now be reinterpreted as special or limiting cases within the broader diffusion framework."

```
Diffusion
    â”‚
    â”œâ”€â”€ VAE = One-step diffusion (ì–‘ë°©í–¥)
    â”‚       ì¸ì½”ë”: ë…¸ì´ì¦ˆ ì¶”ê°€
    â”‚       ë””ì½”ë”: ë””ë…¸ì´ì§•
    â”‚
    â””â”€â”€ GAN = One-step reverse-only diffusion
            Generator: z â†’ x
            ë‹¨ì¼ ìŠ¤í… ê²°ì •ë¡ ì  ë³€í™˜
```

### VAE as Diffusion

```
VAE:
    Encoder: x â†’ z ~ N(Î¼(x), Ïƒ(x)Â²)
    Decoder: z â†’ xÌ‚

Diffusion í•´ì„:
    Forward: xì— ë…¸ì´ì¦ˆ ì¶”ê°€ â†’ z
    Reverse: zì—ì„œ x ë³µì›

ì°¨ì´:
    VAE: ë‹¨ì¼ ìŠ¤í…, í•™ìŠµëœ ë…¸ì´ì¦ˆ
    Diffusion: ë‹¤ë‹¨ê³„, ê³ ì •ëœ ìŠ¤ì¼€ì¤„
```

### GAN as Diffusion

> **ì±… ì›ë¬¸ (p.273):**
> "GANs can be seen as a limiting case of SchrÃ¶dinger bridges... as the noise level Îµ â†’ 0, the bridge converges to a deterministic map."

```
GAN Generator:
    z ~ N(0, I) â†’ x = G(z)

Diffusion í•´ì„:
    T = 1
    g(t) = 0 (ë…¸ì´ì¦ˆ ì—†ìŒ)
    ê²°ì •ë¡ ì  reverse

"OT mapì„ adversarialí•˜ê²Œ í•™ìŠµ"
```

### ê³„ì¸µì  VAE â†’ Discrete Diffusion

```
Ladder VAE:
    z_T â†’ z_{T-1} â†’ ... â†’ z_1 â†’ x

ê° ë ˆì´ì–´:
    z_{t-1} = f(z_t) + noise

T â†’ âˆ:
    ì—°ì† ì‹œê°„ Diffusionìœ¼ë¡œ ìˆ˜ë ´!
```

---

## 4. Dynamic Phase Transitions

### U-Turn Diffusion

> **ì±… ì›ë¬¸ (p.277):**
> "In U-Turn Diffusion, a pre-trained score-based diffusion model is modified by terminating the forward noising process at an intermediate time T_u."

```
U-Turn ì•„ì´ë””ì–´:
    Forward: x_0 â†’ x_{T_u}
    Reverse: x_{T_u} â†’ y_0

T_uì— ë”°ë¥¸ í–‰ë™ ë³€í™”:
    ì‘ì€ T_u: y_0 â‰ˆ x_0 (ê±°ì˜ ë³µì›)
    í° T_u: y_0 â‰ˆ ìƒˆë¡œìš´ ìƒ˜í”Œ
```

### Phase Transitions

```
T_m (Memorization Time):
    ì´ì „: GT ìƒ˜í”Œì— ê°€ê¹Œì›€
    ì´í›„: GTì—ì„œ ë²—ì–´ë‚¨

T_s (Speciation Time):
    ì´ì „: ê°™ì€ í´ë˜ìŠ¤
    ì´í›„: ë‹¤ë¥¸ í´ë˜ìŠ¤ë¡œ ì í”„!
```

### ë¬¼ë¦¬í•™ê³¼ì˜ ì—°ê²°

```
Spin Glass ì´ë¡ :
    - Collapse transition (ì‘ì¶•)
    - Separation transition (ë¶„ë¦¬)

Diffusionì—ì„œ:
    - T_m: íŠ¹ì • ë°ì´í„°ë¡œ ì‘ì¶•
    - T_s: í´ë˜ìŠ¤ ê°„ ë¶„ë¦¬
```

---

## 5. Stochastic Optimal Controlê³¼ RL

### MDP (Markov Decision Process) ê¸°ì´ˆ

$$V^\pi(s) = \mathbb{E}_\pi\left[\sum_{t=0}^\infty \gamma^t r(s_t, a_t) | s_0 = s\right]$$

```
s: ìƒíƒœ (state)
a: í–‰ë™ (action)
r: ë³´ìƒ (reward)
Ï€: ì •ì±… (policy)
V: ê°€ì¹˜ í•¨ìˆ˜ (value function)
```

### Bellman Equation

$$V^*(s) = \max_a \left[r(s, a) + \gamma \mathbb{E}_{s'}[V^*(s')]\right]$$

"ìµœì  ê°€ì¹˜ = ì¦‰ê° ë³´ìƒ + ë¯¸ë˜ ê°€ì¹˜ì˜ ê¸°ëŒ€ê°’"

### Control as Inference

> **ì±… ì›ë¬¸ (p.267):**
> "Physics-inspired priors (e.g. control as inference) enrich classical RL."

```
RL ëª©í‘œ:
    max E[Î£ r_t]

Inference ê´€ì :
    p(Ï„) âˆ exp(Î£ r_t)

"ì¢‹ì€ trajectoryëŠ” ë†’ì€ í™•ë¥ "
â†’ Sampling ë¬¸ì œë¡œ ë³€í™˜!
```

### Diffusion + RL

```
ì„¸ ê°€ì§€ ì—°ê²°:
1. Diffusionì˜ reverse = Stochastic control
2. Score matching = Policy optimization
3. Denoising = Value function approximation
```

---

## 6. Generative Flow Networks (GFlowNets)

### GFlowNets ì•„ì´ë””ì–´

> **ì±… ì›ë¬¸ (p.267):**
> "Generative Flow Networks... samplers over decision trajectories rather than over raw data."

```
ëª©í‘œ: p(x) âˆ R(x)ì—ì„œ ìƒ˜í”Œë§

Trajectory:
    s_0 â†’ s_1 â†’ ... â†’ s_n = x

Flow Matching:
    ê° ìƒíƒœë¡œ ë“¤ì–´ì˜¤ëŠ” flow = ë‚˜ê°€ëŠ” flow

"ë¹„ê°€ì—­ì  ìƒì„± ê³¼ì •ì˜ ì¼ë°˜í™”"
```

### GFlowNets vs Diffusion

| í•­ëª© | GFlowNets | Diffusion |
|------|-----------|-----------|
| **Time axis** | êµ¬ì¡°ì  (ë…¸ë“œ ì¶”ê°€) | ì—°ì†ì  (ë…¸ì´ì¦ˆ) |
| **Reversibility** | ë¹„ê°€ì—­ì  | ê°€ì—­ì  |
| **Structure** | DAG (discrete) | ì—°ì† ê³µê°„ |
| **ì‘ìš©** | ì¡°í•© ìµœì í™” | ì´ë¯¸ì§€ ìƒì„± |

### Decision Flow (GFlowNetsì˜ í™•ì¥)

```
"Diffusion + GFlowNets"

ì—°ì† ì‹œê°„ + êµ¬ì¡°ì  ìƒì„±

ì‘ìš©:
    - ë¶„ì ì„¤ê³„
    - ê°•í™”í•™ìŠµ
    - ì¡°í•© ìµœì í™”
```

---

## 7. Notebooks ê°€ì´ë“œ

### ì£¼ìš” ë…¸íŠ¸ë¶ (ë‹¤ë¥¸ í´ë”ì— ë¶„ì‚°)

| ë…¸íŠ¸ë¶ | ìœ„ì¹˜ | ë‚´ìš© |
|--------|------|------|
| `02-SGM-with-SDE-9grid.ipynb` | chapter9/ | Score-based diffusion |
| `ring_vae_latent_diffusion_comparison.ipynb` | chapter9/ | VAE vs Diffusion |
| `Langevin-DoubleWell.ipynb` | chapter7/ | Langevin dynamics |
| `RBM-MCMC.ipynb` | chapter7/ | Energy-based sampling |

### í•µì‹¬ ì‹¤ìŠµ

**1. Score-Based Diffusion**
```python
# 02-SGM-with-SDE-9grid.ipynb
# 9ê°œ Gaussian modeì—ì„œ diffusion
# Forward/Reverse SDE ì‹œê°í™”
```

**2. U-Turn ì‹¤í—˜**
```python
# T_u ë³€í™”ì— ë”°ë¥¸ ìƒì„± í’ˆì§ˆ
# Memorization vs Generation ê²½ê³„
```

**3. VAE-Diffusion ë¹„êµ**
```python
# ring_vae_latent_diffusion_comparison.ipynb
# ê°™ì€ ë°ì´í„°ì…‹ì—ì„œ ë‘ ë°©ë²• ë¹„êµ
```

---

## 8. ì „ì²´ ì±… í†µí•© ìš”ì•½

### ìˆ˜í•™ì  ê¸°ì´ˆ (Ch.1-3)

```
Ch.1 ì„ í˜•ëŒ€ìˆ˜:
    SVD, ê³ ìœ ê°’ ë¶„í•´, í…ì„œ
    â†’ ë°ì´í„° í‘œí˜„, ì°¨ì› ì¶•ì†Œ

Ch.2 ë¯¸ì ë¶„/ODE:
    ë¯¸ë¶„, Jacobian, ë™ì  ì‹œìŠ¤í…œ
    â†’ Neural ODE, SDE

Ch.3 ìµœì í™”:
    Gradient Descent, ì •ê·œí™”, SGD
    â†’ ì‹ ê²½ë§ í•™ìŠµ
```

### ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ (Ch.4)

```
MLP â†’ CNN â†’ ResNet â†’ Neural ODE â†’ Transformer

í•µì‹¬ í†µì°°:
    ResNet = ODEì˜ ì´ì‚°í™”
    Skip connection = Gradient flow ë³´ì¥
```

### í™•ë¥ ë¡ ì  ê¸°ì´ˆ (Ch.5-6)

```
Ch.5 í™•ë¥ :
    ë¶„í¬, ë³€í™˜, CLT
    â†’ ìƒì„± ëª¨ë¸ì˜ ìˆ˜í•™ì  ê¸°ë°˜

Ch.6 ì •ë³´ ì´ë¡ :
    ì—”íŠ¸ë¡œí”¼, KL, ìƒí˜¸ì •ë³´ëŸ‰
    â†’ VAEì˜ ELBO, Cross-entropy loss
```

### í™•ë¥  ê³¼ì •ê³¼ ìƒ˜í”Œë§ (Ch.7)

```
ìƒ˜í”Œë§: ITS, Importance Sampling
ê³¼ì •: Brownian Motion, SDE, Markov Chain
ë°©ë²•: MCMC, Langevin Dynamics

â†’ Diffusionì˜ ì§ì ‘ì  ê¸°ë°˜!
```

### êµ¬ì¡°ì  ëª¨ë¸ (Ch.8)

```
Energy-Based Models:
    p(x) âˆ exp(-E(x))

Graphical Models:
    ì¡°ê±´ë¶€ ë…ë¦½ êµ¬ì¡°

VAE:
    Variational Inference + Neural Network
```

### ìµœì¢… í†µí•© (Ch.9)

```
Score-Based Diffusion:
    Ch.7 SDE + Ch.4 Neural Net + Ch.6 KL

ëª¨ë“  ìƒì„± ëª¨ë¸ì˜ í†µí•©:
    VAE = One-step diffusion
    GAN = Deterministic bridge
    Flow = ODE-based diffusion

"ëª¨ë“  ê¸¸ì€ Diffusionìœ¼ë¡œ í†µí•œë‹¤"
```

---

## ğŸ“ í•µì‹¬ ì •ë¦¬

### ì´ ì±•í„°ì—ì„œ ê¼­ ê¸°ì–µí•  ê²ƒ

1. **Score Functionì´ í•µì‹¬**
   - $s(x,t) = \nabla_x \log p(x,t)$
   - í™•ë¥ ì´ ì¦ê°€í•˜ëŠ” ë°©í–¥

2. **Forward-Reverse ëŒ€ì‘**
   - Forward: ë…¸ì´ì¦ˆ ì¶”ê°€ (ì •ì˜ë¨)
   - Reverse: Scoreë¡œ ë””ë…¸ì´ì§• (í•™ìŠµ)

3. **ëª¨ë“  ìƒì„± ëª¨ë¸ì˜ í†µí•©**
   - VAE, GAN = Diffusionì˜ íŠ¹ìˆ˜ ê²½ìš°
   - Bridge = ìœ í•œ ì‹œê°„ diffusion

4. **Phase Transitions**
   - T_m: Memorization
   - T_s: Speciation
   - ë¬¼ë¦¬í•™ê³¼ AIì˜ ì—°ê²°

5. **Control + Generation**
   - RL â†” Diffusion
   - GFlowNets: êµ¬ì¡°ì  ìƒì„±

---

## ğŸ”— ì „ì²´ ì±… ì—°ê²°ë„

```
Ch.1 Linear Algebra â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                           â”‚
Ch.2 Calculus/ODE â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
         â”‚                â”‚         â”‚
Ch.3 Optimization â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â†’ Ch.4 Neural Networks
                          â”‚              â”‚
Ch.5 Probability â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â†’ Ch.7 Stochastic Processes
         â”‚                â”‚              â”‚
Ch.6 Information Theory â”€â”€â”¼â”€â†’ Ch.8 Energy-Based Models
                          â”‚              â”‚
                          â””â”€â”€â”€â”€â”€â”€â†’ Ch.9 Synthesis
                                   (Score-Based Diffusion)
```

---

*ì´ ë¬¸ì„œëŠ” Mathematics of Generative AI Book Chapter 9ì˜ í•™ìŠµ ê°€ì´ë“œì´ì ì „ì²´ ì±…ì˜ í†µí•© ìš”ì•½ì…ë‹ˆë‹¤.*
