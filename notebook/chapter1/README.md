# Chapter 1: Linear Algebra (of AI)

> **PDF pages 10-36** | í•µì‹¬: ë²¡í„°, í–‰ë ¬, í…ì„œ, ì»¨ë³¼ë£¨ì…˜, SVD, Transformer
>
> **KAIST Challenge 1-2 ì°¸ì¡°**: [KAIST-challenges-solutions.md](KAIST-challenges-solutions.md)

---

## ëª©ì°¨
1. [ë°ì´í„° í‘œí˜„ì˜ ê¸°ì´ˆ](#11-foundations-of-representing-data-ë°ì´í„°-í‘œí˜„ì˜-ê¸°ì´ˆ)
2. [ì»¨ë³¼ë£¨ì…˜](#12-convolution-ì»¨ë³¼ë£¨ì…˜)
3. [í–‰ë ¬ ë¶„í•´](#13-matrix-decomposition-í–‰ë ¬-ë¶„í•´)
4. [ìƒì„± AI ì‘ìš©](#14-applications-in-generative-ai-ìƒì„±-ai-ì‘ìš©)
5. [ì—°ìŠµë¬¸ì œì™€ í’€ì´](#exercises-ì—°ìŠµë¬¸ì œ)

---

## 1.1 Foundations of Representing Data (ë°ì´í„° í‘œí˜„ì˜ ê¸°ì´ˆ)

### 1.1.1 Vector (ë²¡í„°)

#### ì •ì˜ (ì±… ì›ë¬¸)

> A vector is an ordered collection of numbers that can represent points, directions, or quantities in space.

**ìˆ˜í•™ì  ì •ì˜**: nì°¨ì› ì‹¤ìˆ˜ ê³µê°„ì˜ ìˆœì„œê°€ ìˆëŠ” ìˆ«ì ëª¨ìŒ
$$\mathbf{v} = [v_1, v_2, \ldots, v_n]^\top \in \mathbb{R}^n$$

ì—¬ê¸°ì„œ:
- $v_i$: ë²¡í„°ì˜ ì„±ë¶„ (component)
- $n$: ë²¡í„°ì˜ ì°¨ì› (dimensionality)
- $[\cdot]$: í–‰ë²¡í„° í‘œê¸°
- ${}^\top$: ì „ì¹˜ (í–‰ë²¡í„° â†’ ì—´ë²¡í„°)

#### ì§ê´€ì  ì´í•´

**ë¹„ìœ **: ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë‚˜ì—´í•œ ë¦¬ìŠ¤íŠ¸

```
ì‚¬ëŒì˜ íŠ¹ì§• = [í‚¤(cm), ëª¸ë¬´ê²Œ(kg), ë‚˜ì´]
           = [175, 70, 25]  â† 3ì°¨ì› ë²¡í„°

MNIST ì´ë¯¸ì§€ = [pixel_1, pixel_2, ..., pixel_784]  â† 784ì°¨ì› ë²¡í„°
```

AIì—ê²ŒëŠ” ì´ë¯¸ì§€, ë‹¨ì–´, ì†Œë¦¬ ëª¨ë‘ **ìˆ«ì ë¦¬ìŠ¤íŠ¸(ë²¡í„°)**ë¡œ ë³´ì…ë‹ˆë‹¤.

#### í•µì‹¬ ì—°ì‚° (Key Operations)

| ì—°ì‚° | ìˆ˜ì‹ (ì±… ì›ë¬¸) | ì§ê´€ì  ì„¤ëª… |
|------|---------------|-------------|
| **ë§ì…ˆ** | $(\mathbf{u} + \mathbf{v})_i = u_i + v_i$ | ê°™ì€ ìœ„ì¹˜ë¼ë¦¬ ë”í•˜ê¸° |
| **ìŠ¤ì¹¼ë¼ ê³±** | $(c\mathbf{v})_i = cv_i$ | ë²¡í„° í¬ê¸°ë¥¼ cë°°ë¡œ |
| **ë‚´ì  (Product)** | $\mathbf{u}\mathbf{v}^\top = \sum_{i=1}^n u_i v_i$ | ë‘ ë²¡í„°ì˜ ìœ ì‚¬ë„ ì¸¡ì • |
| **ë…¸ë¦„ (Norm)** | $\|\mathbf{v}\| = \sqrt{\mathbf{v}\mathbf{v}^\top}$ | ë²¡í„°ì˜ ê¸¸ì´ |

#### ë‚´ì ì˜ ê¸°í•˜í•™ì  ì˜ë¯¸

$$\mathbf{u} \cdot \mathbf{v} = \|\mathbf{u}\| \|\mathbf{v}\| \cos\theta$$

- $\theta = 0Â°$: ê°™ì€ ë°©í–¥ â†’ ë‚´ì  ìµœëŒ€ (ì–‘ìˆ˜)
- $\theta = 90Â°$: ì§êµ â†’ ë‚´ì  = 0
- $\theta = 180Â°$: ë°˜ëŒ€ ë°©í–¥ â†’ ë‚´ì  ìµœì†Œ (ìŒìˆ˜)

#### ğŸ’¡ ì™œ í•˜í•„ 'ì½”ì‚¬ì¸(Cosine)'ì¸ê°€? (ì •ë³´ ì „ë‹¬ê³¼ íˆ¬ì˜)

ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•  ë•Œ ì™œ ì½”ì‚¬ì¸ì„ ì“¸ê¹Œìš”? ì´ëŠ” ë²¡í„° ê°„ì˜ **'ë°©í–¥ì„± ì¼ì¹˜ë„'**ë¥¼ ì—ë„ˆì§€ ê³„ëŸ‰í™” ê´€ì ì—ì„œ ë°”ë¼ë³´ëŠ” ê²ƒì…ë‹ˆë‹¤.

**1. ì—ë„ˆì§€ ì „ë‹¬ íš¨ìœ¨ (Projection Efficiency)**
- **ì •ë ¬ë¨ ($0Â°$):** $\cos 0Â° = 1$. ë‘ ë²¡í„°ê°€ ì™„ë²½íˆ ì¼ì¹˜í•˜ì—¬ ì •ë³´/ì—ë„ˆì§€ê°€ 100% ì „ë‹¬ë˜ëŠ” ìƒíƒœ.
- **ì§êµ ($90Â°$):** $\cos 90Â° = 0$. ë‘ ì¶•ì´ ì™„ì „íˆ ë…ë¦½ì (Independent)í•˜ì—¬ í•œìª½ì˜ ë³€í™”ê°€ ë‹¤ë¥¸ ìª½ì— ì•„ë¬´ëŸ° ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ” ìƒíƒœ.
- **ìƒë°˜ë¨ ($180Â°$):** $\cos 180Â° = -1$. ë°©í–¥ì´ ì •ë°˜ëŒ€ì—¬ì„œ ì˜¤íˆë ¤ ì •ë³´ë¥¼ ìƒì‡„í•˜ëŠ” ìƒíƒœ.

**2. ê¸°í•˜í•™ì  ì˜ë¯¸: ì„±ë¶„ ë¶„ì„(Projection)**
$\cos\theta$ëŠ” í•˜ë‚˜ì˜ ë²¡í„°ê°€ ë‹¤ë¥¸ ë²¡í„°ì˜ **ê¸°ì €(Basis)**ë¡œ ì‚¬ìš©ë  ë•Œ, ê·¸ ê¸°ì € ë°©í–¥ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ë§ì€ **ìœ íš¨ ì„±ë¶„**ì„ ê°€ì§€ê³  ìˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

**3. AIì—ì„œ ì½”ì‚¬ì¸ì„ ì“°ëŠ” ì´ìœ : í¬ê¸° ë¶ˆë³€ì„±(Scale Invariance)**
ë‹¨ìˆœ 'ë‚´ì (Dot Product)'ì€ ë²¡í„°ì˜ **ê¸¸ì´(ì—ë„ˆì§€ ì´ëŸ‰)**ì— ë¹„ë¡€í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ AI(íŠ¹íˆ NLP)ì—ì„œëŠ” **ë¹ˆë„ë‚˜ í¬ê¸°(Magnitude)ë³´ë‹¤ ìƒëŒ€ì ì¸ ì •ë³´ì˜ ë°€ë„(Direction)**ë¥¼ ë” ì¤‘ìš”í•˜ê²Œ ë´…ë‹ˆë‹¤.

- **ì˜ˆì‹œ**: ë‹¨ì–´ ì„ë² ë”©ì—ì„œ 'ì• í”Œ'ì´ë¼ëŠ” ë‹¨ì–´ê°€ ë¬¸ì„œ Aì—ëŠ” 1ë²ˆ, ë¬¸ì„œ Bì—ëŠ” 100ë²ˆ ë“±ì¥í•˜ë”ë¼ë„, ë‘ ë¬¸ì¥ì˜ ì£¼ì œ(IT ë˜ëŠ” ê³¼ì¼)ê°€ ê°™ë‹¤ë©´ ë‘ ë¬¸ì¥ì˜ ì£¼ì œ(IT ë˜ëŠ” ê³¼ì¼)ê°€ ê°™ë‹¤ë©´ ë‘ ë²¡í„°ëŠ” ê°™ì€ **ë°©í–¥**ì„ ê°€ë¦¬í‚µë‹ˆë‹¤.
- **ì½”ì‚¬ì¸ ìœ ì‚¬ë„**ëŠ” ë²¡í„°ë¥¼ ë‹¨ìœ„ ê¸¸ì´(Unit length)ë¡œ **ì •ê·œí™”(Normalization)**í•˜ì—¬, ê´€ì¸¡ì¹˜ì˜ ê·œëª¨ì— ê´€ê³„ì—†ì´ ì˜¤ì§ **ì •ë³´ì˜ ë§¥ë½ì  ì¼ì¹˜ë„**ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.

**AI ì‘ìš©**:
- **Word2Vec**: ì„ë² ë”© ê³µê°„ ë‚´ì—ì„œ ë‹¨ì–´ ê°„ì˜ ì˜ë¯¸ì  ìœ ì‚¬ì„±(Semantic Similarity) ì¸¡ì •.
- **Attention**: Queryì™€ Key ì‚¬ì´ì˜ ê´€ê³„ì  ë°€ì ‘ë„ë¥¼ ì¸¡ì •í•˜ì—¬ ì •ë³´ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë°°ë¶„í•˜ëŠ” í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜. (ì—ë„ˆì§€ì˜ ì„ íƒì  ì§‘ì¤‘)

---

### 1.1.2 Matrix (í–‰ë ¬) - ì„ í˜• ë³€í™˜ì˜ í‘œí˜„

#### ì •ì˜ (ì±… ì›ë¬¸)

> A matrix is a 2D array of numbers that generalizes vectors to multiple dimensions.

$$A = \begin{pmatrix}
A_{11} & A_{12} & \cdots & A_{1n} \\
A_{21} & A_{22} & \cdots & A_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
A_{m1} & A_{m2} & \cdots & A_{mn}
\end{pmatrix} \in \mathbb{R}^{m \times n}$$

#### í•µì‹¬ ì—°ì‚° (ì±… ì›ë¬¸ ìˆ˜ì‹)

| ì—°ì‚° | ìˆ˜ì‹ | ì˜ë¯¸ |
|------|------|------|
| **ë§ì…ˆ** | $(A + B)_{ij} = A_{ij} + B_{ij}$ | ì›ì†Œë³„ ë§ì…ˆ |
| **Hadamard ê³±** | $(A \odot B)_{ij} = A_{ij} B_{ij}$ | ì›ì†Œë³„ ê³±ì…ˆ |
| **í–‰ë ¬ ê³±** | $(AB)_{ij} = \sum_{k=1}^n A_{ik} B_{kj}$ | ë³€í™˜ì˜ í•©ì„± |
| **í–‰ë ¬-ë²¡í„° ê³±** | $(A\mathbf{v})_i = \sum_{j=1}^n A_{ij} v_j$ | ë²¡í„° ë³€í™˜ |
| **ì „ì¹˜** | $(A^\top)_{ij} = A_{ji}$ | í–‰â†”ì—´ êµí™˜ |
| **ì—­í–‰ë ¬** | $AA^{-1} = A^{-1}A = I$ | ë³€í™˜ ì·¨ì†Œ |

#### í–‰ë ¬ = ì„ í˜• ë³€í™˜ (Linear Transformation)

> Any linear map $L : \mathbb{R}^n \to \mathbb{R}^m$ can be uniquely expressed in terms of a matrix $A \in \mathbb{R}^{m \times n}$.

**ì±…ì˜ í•µì‹¬ ì˜ˆì‹œ**:

**1. íšŒì „ (Rotation)** - zì¶• ê¸°ì¤€ Î¸ë§Œí¼ íšŒì „:
$$R_z(\theta) = \begin{pmatrix}
\cos\theta & -\sin\theta & 0 \\
\sin\theta & \cos\theta & 0 \\
0 & 0 & 1
\end{pmatrix}$$

**2. ìŠ¤ì¼€ì¼ë§ (Re-scaling)**:
$$S = \begin{pmatrix}
s_x & 0 \\
0 & s_y
\end{pmatrix} \quad \Rightarrow \quad S\begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} s_x x \\ s_y y \end{pmatrix}$$

**3. íˆ¬ì˜ (Projection)** - xì¶•ìœ¼ë¡œ íˆ¬ì˜:
$$P_1 = \begin{pmatrix}
1 & 0 \\
0 & 0
\end{pmatrix} \quad \Rightarrow \quad P_1\begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} x \\ 0 \end{pmatrix}$$

#### ì§ê´€: ì„ í˜• ë³€í™˜ì„ í†µí•œ ê³µê°„ì˜ ì¬êµ¬ì„±

í–‰ë ¬ ì—°ì‚°ì€ ì…ë ¥ ê³µê°„ì˜ ëª¨ë“  ì ì„ ìƒˆë¡œìš´ ì¢Œí‘œê³„ë¡œ ì˜®ê¸°ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ê²©ìê°€ íšŒì „í•˜ê±°ë‚˜, íŠ¹ì • ë°©í–¥ìœ¼ë¡œ ì‹ ì¶•(Scaling)ë˜ê±°ë‚˜, ì°¨ì›ì´ ì••ì¶•(Projection)ë˜ëŠ” ë“±ì˜ ëª¨ë“  ì„ í˜•ì  ë³€í˜•ì„ í•¨ì¶•í•©ë‹ˆë‹¤.

---

### 1.1.3 Tensor (í…ì„œ) - ë‹¤ì°¨ì›ìœ¼ë¡œì˜ ì¼ë°˜í™”

#### ì •ì˜ (ì±… ì›ë¬¸)

> A tensor $T$ of rank $k$ can be represented as:
> $$T \in \mathbb{R}^{d_1 \times d_2 \times \cdots \times d_k}$$
> where $d_i$ is the size along the $i$-th dimension.

#### ë­í¬ (Rank) = ì°¨ì›ì˜ ìˆ˜

| ë­í¬ | ì´ë¦„ | ì˜ˆì‹œ | AI ì‘ìš© |
|------|------|------|---------|
| 0 | Scalar | ì˜¨ë„, Loss ê°’ | ì†ì‹¤ í•¨ìˆ˜ ì¶œë ¥ |
| 1 | Vector | ì†Œë¦¬ íŒŒí˜•, ë‹¨ì–´ ì„ë² ë”© | Word2Vec |
| 2 | Matrix | í‘ë°± ì´ë¯¸ì§€ (HÃ—W) | ê°€ì¤‘ì¹˜ í–‰ë ¬ |
| 3 | 3-Tensor | ì»¬ëŸ¬ ì´ë¯¸ì§€ (HÃ—WÃ—3) | CNN ì…ë ¥ |
| 4 | 4-Tensor | ë¹„ë””ì˜¤ ë°°ì¹˜ (BÃ—HÃ—WÃ—C) | ë°°ì¹˜ í•™ìŠµ |

#### ì±… ì˜ˆì‹œ: RGB ì´ë¯¸ì§€
$$T \in \mathbb{R}^{H \times W \times 3}$$
- H: ë†’ì´ (í–‰ ìˆ˜)
- W: ë„ˆë¹„ (ì—´ ìˆ˜)
- 3: RGB ì±„ë„

#### í…ì„œ ì—°ì‚°: ì •ë³´ì˜ í™•ì¥ê³¼ ì¶•ì•½ (Scale-up & Summary)

AI ëª¨ë¸ì€ ê³ ì°¨ì› ê³µê°„ìœ¼ë¡œ ì •ë³´ë¥¼ **í™•ì¥**í•˜ì—¬ ë³µì¡í•œ ê´€ê³„ë¥¼ í•™ìŠµí•˜ê³ , ë‹¤ì‹œ í•µì‹¬ ì •ë³´ë¡œ **ì¶•ì•½**í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•©ë‹ˆë‹¤.

**1. Direct Product (í…ì„œ ê³±) âŠ— : "ê´€ê³„ì˜ í™•ì¥"**
ë‘ í…ì„œë¥¼ ê²°í•©í•˜ì—¬ ë” ë†’ì€ ë­í¬ì˜ ê´€ê³„ë§ì„ ìƒì„±í•©ë‹ˆë‹¤. ($Rank = k + m$)
$$(T_1 \otimes T_2)_{i_1,\ldots,i_{k+m}} = (T_1)_{i_1,\ldots,i_k} \cdot (T_2)_{i_{k+1},\ldots,i_{k+m}}$$

- **ì˜ë¯¸**: ë‘ ë…ë¦½ì ì¸ ì •ë³´ë¥¼ ê²°í•©í•˜ì—¬ ëª¨ë“  ê²½ìš°ì˜ ìˆ˜ì— ëŒ€í•œ **ìƒí˜¸ì‘ìš© ê³µê°„(Interaction Space)**ì„ í™•ë³´í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
- **AI ì‘ìš©**: íŠ¹ì§•(Feature) ê°„ì˜ ë¹„ì„ í˜•ì  ì¡°í•©ì„ ìƒì„±í•˜ê±°ë‚˜, ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ í†µí•´ ê³ ì°¨ì›ì˜ ê´€ê³„ë¥¼ ì •ì˜í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.

**2. Contraction (ì¶•ì•½) : "ì •ë³´ì˜ íë¦„ê³¼ ìš”ì•½"**
ê³µìœ ëœ ì¸ë±ìŠ¤ë¥¼ ë”°ë¼ ì„±ë¶„ì„ í•©ì‚°í•˜ì—¬ ë­í¬ë¥¼ ê°ì†Œì‹œí‚µë‹ˆë‹¤. ($Rank = k + m - 2$)
$$T_{i_1,i_2,i_4} = \sum_{i_3=1}^{d_3} (T_1)_{i_1,i_2,i_3} \cdot (T_2)_{i_3,i_4}$$

- **ì˜ë¯¸**: íŠ¹ì • ì¶•ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ë³´ë¥¼ í†µí•©(Aggregation)í•˜ì—¬ ì—ë„ˆì§€ë¥¼ ì „ë‹¬í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. **í–‰ë ¬ ê³±**ë„ ì¶•ì•½ì˜ íŠ¹ìˆ˜í•œ í˜•íƒœì…ë‹ˆë‹¤.
- **AI ì‘ìš©**: 
  - **Attention**: Queryì™€ Keyë¥¼ ì¶•ì•½í•˜ì—¬ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ì–»ê³ , ì´ë¥¼ ë‹¤ì‹œ Valueì™€ ì¶•ì•½í•˜ì—¬ ì •ë³´ì˜ ê°€ì¤‘ í•©ì„ êµ¬í•¨.
  - **Layer Transition**: ì´ì „ ì¸µì˜ ì •ë³´ë¥¼ ë‹¤ìŒ ì¸µì˜ ì°¨ì›ìœ¼ë¡œ ë§¤í•‘í•˜ë©° ì •ë³´ë¥¼ ìš”ì•½ ì „ë‹¬.

> [!TIP]
> **AI ëª¨ë¸ì˜ ë™ì‘ ì›ë¦¬**:
> 1. **í™•ì¥(Expansion)**: ë°ì´í„°ë¥¼ ê³ ì°¨ì›(High-dimensional) ê³µê°„ìœ¼ë¡œ ë³´ë‚´ì–´ ì„ í˜•ì ìœ¼ë¡œ ë¶„ë¦¬ ë¶ˆê°€ëŠ¥í•œ ë³µì¡í•œ íŠ¹ì§•ì„ í‘œí˜„í•©ë‹ˆë‹¤.
> 2. **ì¶•ì•½(Contraction)**: ì¤‘ìš”ë„(Attention)ì— ë”°ë¼ ì •ë³´ë¥¼ ì„ ë³„í•˜ê³ , ì°¨ì› ì¶•ì†Œë¥¼ í†µí•´ í•µì‹¬ ì˜ë¯¸(Latent representation)ë¥¼ ë‹¤ìŒ ì¸µìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.

---

### 1.1.4 Einstein Summation Notation (ì•„ì¸ìŠˆíƒ€ì¸ í‘œê¸°ë²•)

> **KAIST Challenge 1** ì§ì ‘ ì—°ê²°

#### ê·œì¹™

**ë°˜ë³µë˜ëŠ” ì¸ë±ìŠ¤ = ìë™ìœ¼ë¡œ í•©ì‚°**

| í‘œê¸° | ìˆ˜ì‹ |
|------|------|
| ì¼ë°˜ í‘œê¸° | $y_i = \sum_j A_{ij} x_j$ |
| Einstein | $y_i = A_{ij} x_j$ (jê°€ ë‘ ë²ˆ â†’ í•©ì‚°) |

#### ì±… Exercise 1.1.3 í’€ì´

**(1) í–‰ë ¬-ë²¡í„° ê³±** $\mathbf{y} = A\mathbf{x}$:
$$y_i = A_{ij} x_j$$

**(2) Frobenius ë…¸ë¦„** $\|A\|_F^2 = \sum_{i,j} A_{ij}^2$:
$$\|A\|_F^2 = A_{ij} A_{ij}$$

#### KAIST Challenge 1 - Question 1

> Given $C_{ikl} = A_{ij}B_{jkl}$, what is the dimensionality of C?

**í’€ì´**:
- A: ì¸ë±ìŠ¤ (i, j) â†’ 2D
- B: ì¸ë±ìŠ¤ (j, k, l) â†’ 3D
- jê°€ ì–‘ìª½ì—ì„œ ë°˜ë³µ â†’ **í•©ì‚°ë˜ì–´ ì‚¬ë¼ì§**
- ë‚¨ëŠ” ì¸ë±ìŠ¤: i, k, l

**ë‹µ: CëŠ” 3ì°¨ì› í…ì„œ (I Ã— K Ã— L)**

#### KAIST Challenge 1 - Question 2: ì—°ì‚°ëŸ‰(FLOPs) vs ë©”ëª¨ë¦¬ ì—°ì†ì„±(Contiguity)

GPU í”„ë¡œê·¸ë˜ë°ê³¼ ë”¥ëŸ¬ë‹ ìµœì í™”ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì‹¤ë¬´ ê°œë…ì…ë‹ˆë‹¤.

- **FLOPs (Floating Point Operations)**: ì´ˆë‹¹ ìˆ˜í–‰ ê°€ëŠ¥í•œ ë¶€ë™ì†Œìˆ˜ì  ì—°ì‚° ìˆ˜. GPUì˜ ìˆœìˆ˜ ì‚°ìˆ  ê³„ì‚° ëŠ¥ë ¥ì„ ëœ»í•©ë‹ˆë‹¤.
- **Memory Bandwidth & Contiguity**: ë©”ëª¨ë¦¬ì—ì„œ ì—°ì‚° ì¥ì¹˜ë¡œ ë°ì´í„°ë¥¼ ì‹¤ì–´ ë‚˜ë¥´ëŠ” ëŒ€ì—­í­ê³¼, ë°ì´í„°ê°€ ë©”ëª¨ë¦¬ ê³µê°„ì— ë¬¼ë¦¬ì ìœ¼ë¡œ ì¸ì ‘í•´ ìˆëŠ” ì •ë„ë¥¼ ëœ»í•©ë‹ˆë‹¤.

**ì™œ ì—°ì†ì„±(Contiguity)ì´ ì„±ëŠ¥ì˜ ë³‘ëª©(Bottleneck)ì´ ë˜ë‚˜ìš”?**
í˜„ëŒ€ GPUëŠ” ì—°ì‚° ì†ë„(Compute)ì— ë¹„í•´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì†ë„(Memory Access)ê°€ ìƒëŒ€ì ìœ¼ë¡œ ë§¤ìš° ëŠë¦½ë‹ˆë‹¤ (**Memory-Bound**).

1. **ë³‘í•© ì ‘ê·¼ (Coalesced Access)**: ë°ì´í„°ê°€ ë©”ëª¨ë¦¬ì— ë‚˜ë€íˆ(Contiguous) ìˆì„ ê²½ìš°, í•œ ë²ˆì˜ ë©”ëª¨ë¦¬ íŠ¸ëœì­ì…˜ìœ¼ë¡œ ì—¬ëŸ¬ ë°ì´í„°ë¥¼ ë™ì‹œì— ì½ì–´ì˜¬ ìˆ˜ ìˆì–´ ì²˜ë¦¬ëŸ‰ì´ ê·¹ëŒ€í™”ë©ë‹ˆë‹¤.
2. **ë¶„ì‚° ì ‘ê·¼ (Non-contiguous)**: ë°ì´í„°ê°€ í©ì–´ì ¸ ìˆì„ ê²½ìš°, ì—°ì‚° ì¥ì¹˜ëŠ” ë°ì´í„°ë¥¼ ê¸°ë‹¤ë¦¬ë©° ê³µíšŒì „(Stall)í•˜ê²Œ ë©ë‹ˆë‹¤. ì•„ë¬´ë¦¬ FLOPsê°€ ë†’ì•„ë„ ë°ì´í„°ê°€ ì œë•Œ ë„ì°©í•˜ì§€ ì•Šìœ¼ë©´ ë¬´ì˜ë¯¸í•©ë‹ˆë‹¤.

**ê²°ë¡ **: 
ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ ìµœì í™”ëŠ” ë‹¨ìˆœíˆ ì—°ì‚° íšŸìˆ˜ë¥¼ ì¤„ì´ëŠ” ê²ƒë³´ë‹¤, ë°ì´í„°ë¥¼ **ë©”ëª¨ë¦¬ ì—°ì†ì„±**ì´ ë³´ì¥ë˜ë„ë¡ ë°°ì¹˜í•˜ì—¬ **ë©”ëª¨ë¦¬ ëŒ€ì—­í­ í™œìš©ë„**ë¥¼ ë†’ì´ëŠ” ê²ƒì´ í›¨ì”¬ ê²°ì •ì ì…ë‹ˆë‹¤.

---

## 1.2 Convolution (ì»¨ë³¼ë£¨ì…˜)

### ì •ì˜ (ì±… ì›ë¬¸)

> Convolution combines two inputs $f$ and $g$, producing an output that reflects their interaction.

**1D ì´ì‚° ì»¨ë³¼ë£¨ì…˜** (ë²¡í„° $\mathbf{x} \in \mathbb{R}^n$, ì»¤ë„ $\mathbf{k} \in \mathbb{R}^m$):
$$y_i = \sum_{j=1}^m k_j \cdot x_{i+j-1}, \quad i = 1, 2, \ldots, n - m + 1$$

### ì»¨ë³¼ë£¨ì…˜ = êµ¬ì¡°í™”ëœ í–‰ë ¬ ê³±

$$\mathbf{y} = A(\mathbf{k}) \cdot \mathbf{x}$$

ì—¬ê¸°ì„œ $A(\mathbf{k})$ëŠ” **Toeplitz í–‰ë ¬**:
$$A(\mathbf{k}) = \begin{pmatrix}
k_1 & 0 & \cdots & 0 \\
k_2 & k_1 & \cdots & 0 \\
\vdots & k_2 & \ddots & \vdots \\
k_m & \vdots & \cdots & k_1 \\
0 & k_m & \ddots & \vdots \\
\vdots & 0 & \cdots & k_m
\end{pmatrix}$$

### 2D ì»¨ë³¼ë£¨ì…˜ (CNNì˜ ê¸°ë³¸)

**ì±… ê³µì‹**:
$$y_{i,j} = \sum_{h,w=1}^m k_{h,w} \cdot x_{i+h-1,j+w-1}$$

### ì±… Example 1.1.2 - ìƒì„¸ ê³„ì‚°

$$\mathbf{x} = \begin{pmatrix}
1 & 2 & 3 & 0 \\
4 & 5 & 6 & 1 \\
7 & 8 & 9 & 0 \\
1 & 0 & 2 & 3
\end{pmatrix}, \quad \mathbf{k} = \begin{pmatrix}
1 & 0 \\
0 & -1
\end{pmatrix}$$

**ê³„ì‚° ê³¼ì •**:

| ìœ„ì¹˜ | ê³„ì‚° | ê²°ê³¼ |
|------|------|------|
| $y_{0,0}$ | $1 \times 1 + 2 \times 0 + 4 \times 0 + 5 \times (-1)$ | $-4$ |
| $y_{0,1}$ | $2 \times 1 + 3 \times 0 + 5 \times 0 + 6 \times (-1)$ | $-4$ |
| $y_{0,2}$ | $3 \times 1 + 0 \times 0 + 6 \times 0 + 1 \times (-1)$ | $2$ |

**ê²°ê³¼**:
$$\mathbf{y} = \begin{pmatrix}
-4 & -4 & 2 \\
-4 & -4 & 6 \\
7 & 6 & 6
\end{pmatrix}$$

### ì™œ ì»¨ë³¼ë£¨ì…˜ì„ ì“°ëŠ”ê°€?

| ì¥ì  | ì„¤ëª… |
|------|------|
| **íŒŒë¼ë¯¸í„° íš¨ìœ¨** | 3Ã—3 ì»¤ë„ = 9ê°œ íŒŒë¼ë¯¸í„°ë¡œ ì „ì²´ ì´ë¯¸ì§€ ì²˜ë¦¬ |
| **ì´ë™ ë¶ˆë³€ì„±** | ê³ ì–‘ì´ê°€ ì–´ë”” ìˆë“  ê°™ì€ í•„í„°ë¡œ ê²€ì¶œ |
| **ê³„ì¸µì  íŠ¹ì§•** | ì‘ì€ íŒ¨í„´ â†’ í° íŒ¨í„´ ì ì§„ì  í•™ìŠµ |

#### ğŸ’¡ ì™œ ê³„ì¸µì  íŠ¹ì§•(Hierarchical Features)ì´ ë‚˜íƒ€ë‚˜ëŠ”ê°€?

ë‹¨ìˆœíˆ ì¸µì„ ìŒ“ëŠ”ë‹¤ê³  íŠ¹ì§•ì´ ê³„ì¸µí™”ë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì»¨ë³¼ë£¨ì…˜ ì—°ì‚°ì˜ ìˆ˜í•™ì /êµ¬ì¡°ì  íŠ¹ì„± ë•Œë¬¸ì— ë°œìƒí•©ë‹ˆë‹¤.

1.  **ìì—°ì˜ êµ¬ì„±ì„±ë¶„ì„± (Compositionality)**: 
    *   ì„¸ìƒì˜ ëª¨ë“  ë¬¼ì²´ëŠ” ê³„ì¸µì ì…ë‹ˆë‹¤. (í”½ì…€ â†’ ì„  â†’ ë„í˜• â†’ ì‚¬ë¬¼ ë¶€í’ˆ â†’ ì „ì²´ ì‚¬ë¬¼)
    *   CNNì€ ì´ ìì—°ìŠ¤ëŸ¬ìš´ ê³„ì¸µ êµ¬ì¡°ë¥¼ ëª¨ë°©í•˜ì—¬ ê°€ì¥ íš¨ìœ¨ì ìœ¼ë¡œ ì„¸ìƒì„ ì´í•´í•©ë‹ˆë‹¤.
2.  **ìˆ˜ìš©ì—­(Receptive Field)ì˜ ëˆ„ì  í™•ì¥**:
    *   ì²« ë²ˆì§¸ ì¸µì˜ 3x3 í•„í„°ëŠ” ì•„ì£¼ ì¢ì€ ì˜ì—­(9ê°œ í”½ì…€)ë§Œ ë´…ë‹ˆë‹¤. ê·¸ëŸ¬ë‹ˆ 'ì„ 'ì´ë‚˜ 'ì ' ê°™ì€ **ë¯¸ì„¸ íŒ¨í„´**ë§Œ ê²€ì¶œ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    *   ì¸µì´ ê¹Šì–´ì§ˆìˆ˜ë¡ ìƒìœ„ ë‰´ëŸ°ì€ í•˜ìœ„ ë‰´ëŸ°ë“¤ì˜ ì •ë³´ë¥¼ ì¢…í•©í•©ë‹ˆë‹¤. ì›ë³¸ ì´ë¯¸ì§€ ê¸°ì¤€ìœ¼ë¡œëŠ” ì ì  ë” ë„“ì€ ë²”ìœ„ë¥¼ í•œëˆˆì— ë³´ê²Œ ë˜ë¯€ë¡œ(ìˆ˜ìš©ì—­ í™•ì¥), 'ì½”', 'ë°”í€´' ê°™ì€ **ê±°ì‹œì  í˜•ì²´**ë¥¼ íŒŒì•…í•˜ê²Œ ë©ë‹ˆë‹¤.
3.  **ì •ë³´ì˜ ì¶”ìƒí™” (Abstraction)**:
    *   ìœ„ë¡œ ì˜¬ë¼ê°ˆìˆ˜ë¡ "ì´ í”½ì…€ì´ ë¹¨ê°„ìƒ‰ì¸ê°€?"ë¼ëŠ” êµ¬ì²´ì  ì •ë³´ëŠ” ë²„ë¦¬ê³ , "ì´ê²ƒì´ ê°•ì•„ì§€ì˜ ê·€ì¸ê°€?"ë¼ëŠ” **í•µì‹¬ ì˜ë¯¸(Semantic)**ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
    *   ë¶ˆí•„ìš”í•œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³  ì‚¬ë¬¼ì˜ ë³¸ì§ˆë§Œ ë‚¨ê¸°ëŠ” ìµœì í™” ê³¼ì •ì˜ ê²°ê³¼ì…ë‹ˆë‹¤.
4.  **íŠ¹ì§• ê³µìœ  (Feature Sharing)**:
    *   í•˜ìœ„ ì¸µì—ì„œ ë°°ìš´ 'ê°€ë¡œ ì„ ' íŠ¹ì§•ì€ ë‚˜ì¤‘ì— ê°•ì•„ì§€ë¥¼ ì°¾ì„ ë•Œë‚˜ ê³ ì–‘ì´ë¥¼ ì°¾ì„ ë•Œ ëª¨ë‘ ì¬ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
    *   ê¸°ì´ˆ ì¬ë£Œë¥¼ ì˜ ë§Œë“¤ì–´ë‘ê³  ì¡°í•©ë§Œ ë‹¤ë¥´ê²Œ í•˜ëŠ” ê²ƒì´ í•™ìŠµ íš¨ìœ¨ ë©´ì—ì„œ ì••ë„ì ìœ¼ë¡œ ìœ ë¦¬í•©ë‹ˆë‹¤.


---

## 1.3 Matrix Decomposition (í–‰ë ¬ ë¶„í•´)

### 1.3.1 SVD (Singular Value Decomposition)

#### ğŸ’¡ SVDë¥¼ ì™œ í•˜ëŠ”ê°€? (Why SVD?)

ë°ì´í„°ëŠ” ë³´í†µ **ì¤‘ë³µ(Redundancy)**ì´ ë§ìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ 'í‚¤'ì™€ 'ë‹¤ë¦¬ ê¸¸ì´'ëŠ” ì„œë¡œ ë°€ì ‘í•˜ê²Œ ê´€ë ¨ë˜ì–´ ìˆì£ . SVDëŠ” ì´ëŸ° ë³µì¡í•œ í‘œë©´ì  ë°ì´í„°ì—ì„œ **ì ì¬ëœ í•µì‹¬ íŠ¹ì§•(Latent Features)**ì„ ì¶”ì¶œí•˜ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.

- **ë°ì´í„° ì••ì¶•**: ì¤‘ìš”ë„ê°€ ë‚®ì€ ì •ë³´ë¥¼ ë²„ë¦¬ê³  í•µì‹¬ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
- **ë…¸ì´ì¦ˆ ì œê±°**: ì‘ì€ íŠ¹ì´ê°’(Singular Value)ì€ ë³´í†µ ë…¸ì´ì¦ˆì´ë¯€ë¡œ ì´ë¥¼ ì œê±°í•˜ì—¬ ë°ì´í„°ë¥¼ ì •ì œí•©ë‹ˆë‹¤.
- **ì¶”ì²œ ì‹œìŠ¤í…œ**: ì‚¬ìš©ìì˜ ì·¨í–¥($U$)ê³¼ ì•„ì´í…œì˜ íŠ¹ì„±($V$)ì„ ì—°ê²°í•˜ëŠ” í•µì‹¬ êµ¬ì¡°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.

#### ì •ì˜ (Definition)

ì„ì˜ì˜ $m \times n$ í–‰ë ¬ $X$ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì„¸ í–‰ë ¬ì˜ ê³±ìœ¼ë¡œ ë¶„í•´í•˜ëŠ” ê²ƒì„ SVDë¼ê³  í•©ë‹ˆë‹¤.

$$X = U \Sigma V^\top$$

- $U \in \mathbb{R}^{m \times m}$: ì™¼ìª½ íŠ¹ì´ë²¡í„° (**Left** Singular Vectors) - ìƒ˜í”Œ(í–‰) ê°„ì˜ ê´€ê³„
- $\Sigma \in \mathbb{R}^{m \times n}$: íŠ¹ì´ê°’ ëŒ€ê°í–‰ë ¬ (**Singular Values**) - íŠ¹ì§•ì˜ ì¤‘ìš”ë„ ($\sigma_1 \geq \sigma_2 \geq \dots$). **í‘œì¤€í¸ì°¨(Standard Deviation)**ì™€ ê°™ì€ ìŠ¤ì¼€ì¼ì…ë‹ˆë‹¤.
- $V \in \mathbb{R}^{n \times n}$: ì˜¤ë¥¸ìª½ íŠ¹ì´ë²¡í„° (**Right** Singular Vectors) - íŠ¹ì§•(ì—´) ê°„ì˜ ê´€ê³„

> [!NOTE]
> **ì™œ íŠ¹ì§•(Feature)ì´ ì—´(Column)ì¸ê°€ìš”?**
> ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ì˜ í‘œì¤€ ì•½ì†(Convention)ì…ë‹ˆë‹¤.
> - **í–‰(Row)**: ê°œë³„ ê´€ì¸¡ì¹˜ (ì˜ˆ: ì‚¬ëŒ A, ì‚¬ëŒ B, ì´ë¯¸ì§€ 1) â†’ **ìƒ˜í”Œ ê³µê°„**
> - **ì—´(Column)**: ê° ê´€ì¸¡ì¹˜ì˜ ì†ì„± (ì˜ˆ: í‚¤, ëª¸ë¬´ê²Œ, í”½ì…€ê°’) â†’ **íŠ¹ì§• ê³µê°„**
>
> **Column-space representationì˜ ì˜ë¯¸**:
> í–‰ë ¬ $X$ì˜ ê° ì—´ì´ í•˜ë‚˜ì˜ íŠ¹ì§•ì„ ë‚˜íƒ€ë‚´ë¯€ë¡œ, ì´ ì—´ë“¤ì„ ì¡°í•©í•´ì„œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ëª¨ë“  ê³µê°„(ì—´ ê³µê°„)ì€ ê³§ **"íŠ¹ì§•ë“¤ì´ ìƒí˜¸ì‘ìš©í•˜ì—¬ ë§Œë“¤ ìˆ˜ ìˆëŠ” ëª¨ë“  ê°€ëŠ¥ì„±"**ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. $V$ëŠ” ì´ ë³µì¡í•œ ì›ë³¸ íŠ¹ì§•ë“¤ì˜ ì‹¤íƒ€ë˜ë¥¼ í’€ì–´, ì„œë¡œ ê²¹ì¹˜ì§€ ì•ŠëŠ”(ì§êµí•˜ëŠ”) ìƒˆë¡œìš´ **'ì ì¬ íŠ¹ì§•(Latent Feature)'** ì¶•ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤.


#### ğŸ’¡ SVDì˜ ë¬¼ë¦¬ì  ì˜ë¯¸: ë°ì´í„°ì˜ ì§êµ ê¸°ì € ë¶„í•´

í–‰ë ¬ $X$ë¥¼ SVDë¡œ ë¶„í•´í•œë‹¤ëŠ” ê²ƒì€, ë³µì¡í•˜ê²Œ ë’¤ì—‰í‚¨ ë°ì´í„°ë¥¼ **ê°€ì¥ ì„¤ëª…ë ¥ì´ ë†’ì€ ë…ë¦½ì ì¸ ì¶•(Orthogonal Basis)**ë“¤ë¡œ ë‹¤ì‹œ ì •ë ¬í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

- **$U$ (Left Singular Vectors)**: ìƒ˜í”Œ ê³µê°„ì˜ ê¸°ì €. ë°ì´í„° í¬ì¸íŠ¸ë“¤ì´ ìƒˆë¡œìš´ íŠ¹ì§• ì¶• ìœ„ì—ì„œ ì–´ë–¤ ì¢Œí‘œë¥¼ ê°–ëŠ”ì§€ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. (Row-space representation)
- **$\Sigma$ (Singular Values)**: ê° ì¶•ì˜ **ì¤‘ìš”ë„(Impact)**. ë°ì´í„°ê°€ í•´ë‹¹ ì¶• ë°©í–¥ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ë„“ê²Œ í¼ì ¸ ìˆëŠ”ì§€(Scale)ë¥¼ ë‚˜íƒ€ë‚´ë©°, í†µê³„í•™ì˜ **í‘œì¤€í¸ì°¨(Standard Deviation)**ì™€ ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘ë©ë‹ˆë‹¤.
- **$V$ (Right Singular Vectors)**: ë³€ìˆ˜ ê³µê°„ì˜ ê¸°ì €. ì›ë³¸ íŠ¹ì§•ë“¤ì´ ì¡°í•©ë˜ì–´ ì–´ë–¤ ìƒˆë¡œìš´ 'ì ì¬ íŠ¹ì§•(Latent Feature)'ì„ í˜•ì„±í•˜ëŠ”ì§€ ì •ì˜í•©ë‹ˆë‹¤. (Column-space representation)

**ì„±ì§ˆ**: $U$ì™€ $V$ì˜ ì—´ë²¡í„°ë“¤ì€ ì„œë¡œ ì§êµ(Orthogonal)í•©ë‹ˆë‹¤. ì´ëŠ” SVDê°€ ë°ì´í„°ë¥¼ **ì„œë¡œ ì¤‘ë³µë˜ì§€ ì•ŠëŠ”(Redundancy-free) ì •ë³´ì˜ í•©**ìœ¼ë¡œ ìª¼ê°œì¤€ë‹¤ëŠ” ê²ƒì„ ë³´ì¥í•©ë‹ˆë‹¤.

#### ğŸ’¡ íŠ¹ì´ê°’($\Sigma$, Singular Value)ì´ 'íŠ¹ì§•ì˜ ê°•ë„'ì¸ ì´ìœ 

1. **ë¬¼ë¦¬ì  ì‹ ì¶• (Scaling Factor)**: ì„ í˜• ë³€í™˜ $X$ë¥¼ ê¸°í•˜í•™ì ìœ¼ë¡œ í•´ì„í•˜ë©´, ì…ë ¥ ë²¡í„° $V$ê°€ $X$ë¥¼ í†µê³¼í•œ í›„ ê²°ê³¼ ë²¡í„° $U$ë¡œ ë³€í•  ë•Œ ê·¸ **ê¸¸ì´ê°€ $\sigma$ë°°ë§Œí¼ ë³€í™”**í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤ ($Xv = \sigma u$). ì¦‰, $\sigma$ëŠ” í•´ë‹¹ ë°©í–¥ìœ¼ë¡œ ê³µê°„ì´ í™•ì¥ë˜ëŠ” ì •ë„ë¥¼ ì •ëŸ‰í™”í•œ ìˆ˜ì¹˜ì…ë‹ˆë‹¤.
2. **ì‹ í˜¸ ëŒ€ ì¡ìŒë¹„ (Signal-to-Noise Ratio)**:
   - í° íŠ¹ì´ê°’($\sigma$): ë°ì´í„°ì˜ ì£¼ìš” ê³¨ê²©(Signal)ì„ í˜•ì„±í•˜ëŠ” ì§€ë°°ì ì¸ íŠ¹ì§•.
   - ì‘ì€ íŠ¹ì´ê°’($\sigma$): ë°ì´í„°ì˜ ë¯¸ì„¸í•œ ë³€ë™ì´ë‚˜ ë¬´ì‘ìœ„ ì˜¤ì°¨(Noise)ë¡œ ê°„ì£¼ë  ìˆ˜ ìˆëŠ” ë¶€ì°¨ì  ì„±ë¶„.
3. **í†µê³„ì  ì—°ê²° (Standard Deviation)**: ë°ì´í„°ê°€ íŠ¹ì • ë°©í–¥ìœ¼ë¡œ í¼ì§„ ì •ë„ë¥¼ ë‹¤ì°¨ì› ê³µê°„ì—ì„œ ì¸¡ì •í•œ ê°’ì…ë‹ˆë‹¤. ì´ëŠ” í›„ìˆ í•  ê³µë¶„ì‚° í–‰ë ¬ì˜ **ë¶„ì‚°($\sigma^2$)** ê°œë…ê³¼ ì œê³± ê´€ê³„ë¥¼ ê°€ì§‘ë‹ˆë‹¤.

> **ê²°ë¡ **: íŠ¹ì´ê°’ì€ **"í•´ë‹¹ ì ì¬ ì¶•ì´ ì „ì²´ ë°ì´í„°ë¥¼ ì„¤ëª…í•˜ëŠ” ë° ê¸°ì—¬í•˜ëŠ” ì •ë„(Explanatory Power)"**ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤.

#### ï¿½ ê¸°í•˜í•™ì  ì˜ë¯¸: ê³µê°„ì˜ ë³€í˜•

SVDëŠ” ì–´ë–¤ ì„ í˜• ë³€í™˜ì´ë“  **"íšŒì „ $\to$ ëŠ˜ë¦¬ê¸° $\to$ íšŒì „"**ìœ¼ë¡œ ìª¼ê°¤ ìˆ˜ ìˆìŒì„ ë§í•´ì¤ë‹ˆë‹¤.

1.  $V^\top$: ë°ì´í„°ì˜ ì£¼ì¶•ì„ ë§ì¶”ê¸° ìœ„í•´ **íšŒì „**ì‹œí‚µë‹ˆë‹¤.
2.  $\Sigma$: ê° ì¶•ì„ ì¤‘ìš”ë„(íŠ¹ì´ê°’)ì— ë”°ë¼ **ëŠ˜ë¦¬ê±°ë‚˜ ì¤„ì…ë‹ˆë‹¤**.
3.  $U$: ë³€í˜•ëœ ë°ì´í„°ë¥¼ ìµœì¢… ë„ì°©ì§€ ê³µê°„ìœ¼ë¡œ ë‹¤ì‹œ **íšŒì „**ì‹œí‚µë‹ˆë‹¤.


#### âœï¸ ìˆ˜í•™ì  ì¦ëª…: ì–´ë–»ê²Œ ì´ë ‡ê²Œ ìª¼ê°œì§€ëŠ”ê°€?

SVDê°€ ë¬´ì¡°ê±´ ì¡´ì¬í•  ìˆ˜ë°–ì— ì—†ëŠ” ì´ìœ ëŠ” $X^\top X$ë¼ëŠ” ëŒ€ì¹­í–‰ë ¬ì˜ ì„±ì§ˆì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤.

1.  **ì…ë ¥ ì¶•($V$) ì„ íƒ**: $X^\top X$ëŠ” ëŒ€ì¹­í–‰ë ¬ì´ë¯€ë¡œ í•­ìƒ ì„œë¡œ ìˆ˜ì§ì¸ ê³ ìœ ë²¡í„° $v_1, v_2, \dots$ë¥¼ ê°€ì§‘ë‹ˆë‹¤. ì´ë¥¼ $V$ë¼ê³  í•©ë‹ˆë‹¤.
    $$X^\top X v_i = \lambda_i v_i$$
2.  **ë³€í™˜ í›„ í¬ê¸°($\sigma_i$) ì¸¡ì •**: ì´ ì¶• $v_i$ì— $X$ë¥¼ ê³±í–ˆì„ ë•Œ ê²°ê³¼ë¬¼ $X v_i$ì˜ ê¸¸ì´ë¥¼ $\sigma_i$ë¼ê³  ì •ì˜í•©ë‹ˆë‹¤.
    $$\sigma_i = \|X v_i\| = \sqrt{\lambda_i}$$
3.  **ê²°ê³¼ ì¶•($U$) ê²°ì •**: ë³€í™˜ëœ ê²°ê³¼ë¬¼ì˜ ë°©í–¥ì„ $u_i$ë¼ê³  ì´ë¦„ ë¶™ì…ë‹ˆë‹¤.
    $$u_i = \frac{X v_i}{\sigma_i} \quad \Rightarrow \quad X v_i = \sigma_i u_i$$
4.  **í–‰ë ¬ ì¡°ë¦½**: ìœ„ ê´€ê³„ë“¤ì„ ë¬¶ìœ¼ë©´ $X V = U \Sigma$ ê°€ ë˜ë©°, ì–‘ë³€ì— $V^\top$ë¥¼ ê³±í•˜ë©´ ìµœì¢… ì‹ì´ ì™„ì„±ë©ë‹ˆë‹¤.
    $$X = U \Sigma V^\top$$

> **í•µì‹¬**: "ì…ë ¥ì—ì„œ ìˆ˜ì§ì¸ ì¶•($V$)ì„ ì˜ ê³¨ëë”ë‹ˆ, ë³€í™˜ í›„ì—ë„ ê·¸ ì¶•ë“¤ì´ ìˆ˜ì§($U$)ì„ ìœ ì§€í•˜ë©° ì˜ˆì˜ê²Œ ìª¼ê°œì§€ë”ë¼!"

#### ï¿½ï¸ SVDì˜ ìˆ˜í•™ì  ë³´ì¦ (The Mathematical Guarantee)

SVDë¥¼ í•  ë•Œ ìš°ë¦¬ê°€ **"ë¬´ì¡°ê±´ ë¶„í•´í•  ìˆ˜ ìˆë‹¤"**ê³  í™•ì‹ í•  ìˆ˜ ìˆëŠ” ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜í•™ì  ì¥ì¹˜ ë•Œë¬¸ì…ë‹ˆë‹¤.

1.  **ë°ì´í„° $X$ëŠ” ì‹¤ìˆ˜ì…ë‹ˆë‹¤**: ìš°ë¦¬ê°€ ë‹¤ë£¨ëŠ” ë°ì´í„°ëŠ” í˜„ì‹¤ì˜ ìˆ«ì(ì‹¤ìˆ˜)ì…ë‹ˆë‹¤.
2.  **$X^\top X$ëŠ” ë¬´ì¡°ê±´ ëŒ€ì¹­í–‰ë ¬ì…ë‹ˆë‹¤**: ì£¼ëŒ€ê°ì„ ì„ ê¸°ì¤€ìœ¼ë¡œ ì ‘ì—ˆì„ ë•Œ ìˆ«ìê°€ ì¼ì¹˜í•˜ëŠ” **ëŒ€ê° ëŒ€ì¹­** í–‰ë ¬ì´ ë©ë‹ˆë‹¤. ($A^\top = A$)
3.  **ìŠ¤í™íŠ¸ëŸ¼ ì •ë¦¬ (Spectral Theorem)**: "ëª¨ë“  ì‹¤ìˆ˜ ëŒ€ì¹­í–‰ë ¬ì€ **ì„œë¡œ ìˆ˜ì§ì¸ ê³ ìœ ë²¡í„°($V$)**ë¥¼ ê°€ì§„ë‹¤"ëŠ” ê²ƒì´ ìˆ˜í•™ì ìœ¼ë¡œ ì´ë¯¸ ì¦ëª…ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

> [!TIP]
> ì—¬ê¸°ì„œ **ëŒ€ì¹­**ì€ ê°€ë¡œì¶• ëŒ€ì¹­ì´ ì•„ë‹ˆë¼ **ì£¼ëŒ€ê°ì„ (ì™¼ìª½ ìœ„ $\to$ ì˜¤ë¥¸ìª½ ì•„ë˜) ëŒ€ì¹­**ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì „ì¹˜ ì—°ì‚°ì˜ ì„±ì§ˆ($(X^\top X)^\top = X^\top X$) ë•ë¶„ì— ì–´ë–¤ ë°ì´í„° $X$ë¥¼ ê°€ì ¸ì™€ë„ $X^\top X$ëŠ” ë¬´ì¡°ê±´ ì´ ì˜ˆìœ ëŒ€ì¹­ êµ¬ì¡°ë¥¼ ê°€ì§€ê²Œ ë©ë‹ˆë‹¤.

#### ğŸ”— SVDì™€ ê³ ìœ ê°’ ë¶„í•´ì˜ ì—°ê²° (Eigen-decomposition)

SVDëŠ” ì •ë°©í–‰ë ¬ì´ ì•„ë‹Œ í–‰ë ¬ì—ë„ ì ìš© ê°€ëŠ¥í•œ **ê³ ìœ ê°’ ë¶„í•´ì˜ ì¼ë°˜í™”**ì…ë‹ˆë‹¤.

- **ê´€ê³„ì‹**: $X^\top X = (V \Sigma^\top U^\top) (U \Sigma V^\top) = V (\Sigma^\top \Sigma) V^\top = V \Sigma^2 V^\top$
- $X^\top X$ì˜ ê³ ìœ ë²¡í„° = $V$ (ì˜¤ë¥¸ìª½ íŠ¹ì´ë²¡í„°)
- $X^\top X$ì˜ ê³ ìœ ê°’ $\lambda_i$ = $\sigma_i^2$ (**íŠ¹ì´ê°’ì˜ ì œê³±**)

#### ğŸ’¡ ì™œ ì œê³±($^2$)ì´ ë˜ëŠ”ê°€? (Mapping $S \to S^2$)

ì´ ê´€ê³„ëŠ” ë°ì´í„°ì˜ **ìŠ¤ì¼€ì¼** ë³€í™”ë¥¼ ì´í•´í•˜ëŠ” í•µì‹¬ì…ë‹ˆë‹¤.
> 1. **SVD ($X = USV^\top$)**: ë°ì´í„° ìì²´ë¥¼ ë¶„í•´í•©ë‹ˆë‹¤. ì´ë•Œ íŠ¹ì´ê°’($s$)ì€ ë°ì´í„°ê°€ ê·¸ ì¶•ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ê¸¸ê²Œ ë»—ì–´ìˆëŠ”ì§€, ì¦‰ **í‘œì¤€í¸ì°¨(Standard Deviation)**ì˜ ê·œëª¨ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
> 2. **Eigen ($C = V \Lambda V^\top$)**: ë°ì´í„°ì˜ ê³µë¶„ì‚°(ë³€ë™ì„±)ì„ ë¶„í•´í•©ë‹ˆë‹¤. ê³µë¶„ì‚°ì€ ê°’ë“¤ì˜ ê³±($X^\top X$)ìœ¼ë¡œ ê³„ì‚°ë˜ë¯€ë¡œ, ê²°ê³¼ì ìœ¼ë¡œ ìŠ¤ì¼€ì¼ ì—­ì‹œ ì œê³±ì¸ **ë¶„ì‚°(Variance)**ì˜ ê·œëª¨($s^2$)ë¡œ ë³€í•©ë‹ˆë‹¤.

> **ì •ë¦¬**: "ë°ì´í„°ì˜ ê¸¸ì´(SVD)ë¥¼ ê³±í–ˆë”ë‹ˆ, ë°ì´í„°ì˜ ì—ë„ˆì§€(Eigen)ê°€ ë˜ì—ˆë‹¤!"

#### ğŸ“‰ Low-Rank Approximation (ì°¨ì› ì¶•ì†Œ)

$$X \approx U_k \Sigma_k V_k^\top$$

ìƒìœ„ $k$ê°œ íŠ¹ì´ê°’ë§Œ ì‚¬ìš©í•˜ë©´ ë°ì´í„°ì˜ ë¼ˆëŒ€ë§Œ ë‚¨ê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¶”ì²œ ì‹œìŠ¤í…œì´ë‚˜ ì´ë¯¸ì§€ ì••ì¶•ì—ì„œ ê°€ì¥ í•µì‹¬ì´ ë˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.

#### KAIST Challenge 2 ì—°ê²°

> **Exercise**: 90% ì—ë„ˆì§€ë¥¼ ìœ ì§€í•˜ëŠ” ìµœì†Œ $k$ ì°¾ê¸°

**ì´ ì—ë„ˆì§€**: $E_{total} = \sum_{i=1}^{r} \sigma_i^2 = \|X\|_F^2$

**90% ì¡°ê±´**:
$$\frac{\sum_{i=1}^{k} \sigma_i^2}{\sum_{i=1}^{r} \sigma_i^2} \geq 0.90$$

**Question 1**: $k$ ì„ íƒ = ë©”ëª¨ë¦¬ vs ì •ë³´ ì†ì‹¤ íŠ¸ë ˆì´ë“œì˜¤í”„
- $k$ ì‘ìŒ: ë©”ëª¨ë¦¬ ì ìŒ, ì •ë³´ ì†ì‹¤ í¼
- $k$ í¼: ë©”ëª¨ë¦¬ ë§ìŒ, ì •ë³´ ì†ì‹¤ ì ìŒ

**Question 2**: ì™œ $X^\top X$ ê³ ìœ ë¶„í•´ê°€ íš¨ìœ¨ì ì¸ê°€?

| ë°©ë²• | í–‰ë ¬ í¬ê¸° | N=100,000, D=100 |
|------|-----------|------------------|
| $X$ ì§ì ‘ SVD | $N \times D$ | 80GB ë©”ëª¨ë¦¬ |
| $X^\top X$ ê³ ìœ ë¶„í•´ | $D \times D$ | 80KB ë©”ëª¨ë¦¬ |

> **ì´ìœ **: PCAì—ì„œëŠ” $V$(ì£¼ì„±ë¶„)ë§Œ í•„ìš”í•˜ê³  ê° ìƒ˜í”Œì˜ ìœ„ì¹˜ì¸ $U$ëŠ” ë‚˜ì¤‘ì— ê³„ì‚°í•˜ê±°ë‚˜ ë¶ˆí•„ìš”í•œ ê²½ìš°ê°€ ë§ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.


---

### 1.3.2 Eigen-Decomposition (ê³ ìœ ê°’ ë¶„í•´)

#### ì •ì˜ (ì±… ì›ë¬¸)

> An eigenvalue-eigenvector pair $(\lambda, \mathbf{v})$ satisfies:
> $$A\mathbf{v} = \lambda \mathbf{v}$$

ì •ë°©í–‰ë ¬ $A$ì˜ ë¶„í•´:
$$A = Q \Lambda Q^{-1}$$

- $Q$: ê³ ìœ ë²¡í„° í–‰ë ¬
- $\Lambda$: ê³ ìœ ê°’ ëŒ€ê°í–‰ë ¬

#### ì§ê´€: "ë°©í–¥"ê³¼ "ë°°ìœ¨"ì˜ ì°¨ì´

ëŒ€ë¶€ë¶„ì˜ ë²¡í„°ëŠ” í–‰ë ¬ $A$ë¥¼ ë§Œë‚˜ë©´ ë°©í–¥ë„ ë°”ë€Œê³  í¬ê¸°ë„ ë°”ë€ë‹ˆë‹¤. í•˜ì§€ë§Œ **ê³ ìœ ë²¡í„°**ëŠ” ë°©í–¥ì„ ê¿‹ê¿‹ì´ ì§€í‚¤ê³  í¬ê¸°ë§Œ ë³€í•˜ëŠ” íŠ¹ë³„í•œ ë²¡í„°ì…ë‹ˆë‹¤.

- **ê³ ìœ ë²¡í„° (Eigenvector)**: **"ì–´ë””ë¡œ?"** (ë°©í–¥) - ë³€í™˜ í›„ì—ë„ ì‚´ì•„ë‚¨ì€ ë°ì´í„°ì˜ í•µì‹¬ ì¶•
- **ê³ ìœ ê°’ (Eigenvalue)**: **"ì–¼ë§ˆë‚˜ ë§ì´?"** (ë°°ìœ¨) - ê·¸ ì¶•ì˜ ì˜í–¥ë ¥ ë˜ëŠ” ì—ë„ˆì§€ì˜ í¬ê¸°

> [!NOTE]
> **SVDì™€ì˜ ì—°ê²°**: SVDì˜ **íŠ¹ì´ê°’($\sigma$)**ì€ ê³ ìœ ê°’ì˜ í˜•ì œì…ë‹ˆë‹¤. ë°ì´í„° í–‰ë ¬ $X$ì—ì„œ $X^\top X$ì˜ **ê³ ìœ ê°’**ì„ êµ¬í•œ ë’¤ ë£¨íŠ¸($\sqrt{\cdot}$)ë¥¼ ì”Œìš°ë©´ ë°”ë¡œ SVDì˜ **íŠ¹ì´ê°’**ì´ ë©ë‹ˆë‹¤.


#### ğŸ’¡ ê³µë¶„ì‚° í–‰ë ¬($C = \frac{1}{n}X^\top X$)ê³¼ ê³ ìœ ë¶„í•´ì˜ ì˜ë¯¸

ë°ì´í„° ë¶„ì„(PCA ë“±)ì—ì„œ ì™œ í•˜í•„ ê³µë¶„ì‚° í–‰ë ¬ $C$ë¥¼ ê³ ìœ ë¶„í•´í•˜ëŠ”ì§€ ì´í•´í•˜ë©´ ì„ í˜•ëŒ€ìˆ˜ì˜ ì ˆë°˜ì„ ì´í•´í•œ ê²ƒì…ë‹ˆë‹¤.

1. **í–‰ë ¬ $C$ì˜ ì •ì²´**: ë°ì´í„° $X$ë¥¼ ê³±í•´ ë§Œë“  $C$ëŠ” ë°ì´í„°ì˜ **'í¼ì§ì„±(ë³€ë™ì„±)'**ì„ ë‹´ê³  ìˆëŠ” ì§€ë„ì…ë‹ˆë‹¤.
2. **ê³ ìœ ë²¡í„° ($V$) = ë°ì´í„°ì˜ 'ì£¼ì¶•'**:
   - $C\mathbf{v} = \lambda\mathbf{v}$ ë¥¼ ë§Œì¡±í•˜ëŠ” $V$ëŠ” ë°ì´í„°ê°€ ê°€ì¥ ê¸¸ê²Œ ëŠ˜ì–´ì§„ **ë°©í–¥**ì„ ê°€ë¦¬í‚µë‹ˆë‹¤.
   - ì¦‰, "ë°ì´í„°ê°€ ì–´ëŠ ë°©í–¥ìœ¼ë¡œ ê°€ì¥ ë§ì´ ë³€í•˜ëŠ”ê°€?"ì— ëŒ€í•œ ë‹µì…ë‹ˆë‹¤.
3. **ê³ ìœ ê°’ ($\lambda$) = ë°ì´í„°ì˜ 'ë³€ì‚°ì„±(ì—ë„ˆì§€)'**:
   - í•´ë‹¹ ë°©í–¥ìœ¼ë¡œ ë°ì´í„°ê°€ ì–¼ë§ˆë‚˜ **ë§ì´ í¼ì ¸ ìˆëŠ”ì§€**ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
   - $\lambda$ê°€ í´ìˆ˜ë¡ ê·¸ ë°©í–¥ì€ ë°ì´í„°ë¥¼ ì„¤ëª…í•˜ëŠ” ë° ì¤‘ìš”í•œ ì •ë³´(ì£¼ì„±ë¶„)ê°€ ë©ë‹ˆë‹¤.

> **ë¬¼ë¦¬ì  í•´ì„**: ë°ì´í„°ê°€ ë‹¤ë³€ëŸ‰ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¼ ë•Œ, ê³ ìœ ë²¡í„°ëŠ” ë°ì´í„° ë¶„í¬ì˜ **ì£¼ì¶•(Principal Axis)** ë°©í–¥ì„ ê°€ë¦¬í‚¤ê³ , ê³ ìœ ê°’ì€ ê·¸ ì¶• ë°©í–¥ìœ¼ë¡œì˜ **ì—ë„ˆì§€(ë¶„ì‚°)** í¬ê¸°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

#### SVDì™€ Eigenì˜ ì—°ê²° (ì±… ì›ë¬¸)

> For symmetric positive-definite matrices, such as the covariance matrix $\Sigma = \frac{1}{n} X^\top X$:

$$\Sigma = V \Lambda V^\top = V S^2 V^\top$$

| í•­ëª© | SVD | Eigen |
|------|-----|-------|
| ì ìš© ëŒ€ìƒ | ì„ì˜ì˜ mÃ—n í–‰ë ¬ | ì •ë°©í–‰ë ¬ (nÃ—n) |
| ë¶„í•´ | $X = U\Sigma V^\top$ | $A = Q\Lambda Q^{-1}$ |
| ì§êµì„± | U, V í•­ìƒ ì§êµ | ëŒ€ì¹­ì¼ ë•Œë§Œ ì§êµ |

**í•µì‹¬ ì—°ê²°**:
- $X^\top X$ì˜ ê³ ìœ ë²¡í„° = Xì˜ ì˜¤ë¥¸ìª½ íŠ¹ì´ë²¡í„° V
- $XX^\top$ì˜ ê³ ìœ ë²¡í„° = Xì˜ ì™¼ìª½ íŠ¹ì´ë²¡í„° U

#### ğŸ“Š SVD vs Eigen í•œëˆˆì— ë³´ê¸° (Integrated Summary)

| ê°œë… | ê¸°í˜¸ | ìŠ¤ì¼€ì¼ | ë¬¼ë¦¬ì  ì˜ë¯¸ | AI/ë°ì´í„° ì˜ë¯¸ |
| :--- | :--- | :--- | :--- | :--- |
| **íŠ¹ì´ê°’ (SVD)** | $s$ or $\sigma$ | **í‘œì¤€í¸ì°¨ (SD)** | ë°ì´í„°ê°€ ë³´ì¡´ëœ ë¬¼ë¦¬ì  ê¸¸ì´ | íŠ¹ì§•ì˜ ì˜í–¥ë ¥ (ê°•ë„) |
| **ê³ ìœ ê°’ (Eigen)** | $\lambda = s^2$ | **ë¶„ì‚° (Var)** | ë°ì´í„°ê°€ ê°€ì§„ ì´ ì—ë„ˆì§€ | ì •ë³´ëŸ‰ / ë³€ë™ì„± |
| **Whitening** | $\Sigma^{-1}$ | **1 / í‘œì¤€í¸ì°¨** | ë°ì´í„°ë¥¼ 1(unit)ë¡œ ì •ê·œí™” | ë…¸ì´ì¦ˆí™” (Isotropic) |

---

### 1.3.3 Reduced Representation (ì°¨ì› ì¶•ì†Œ)

#### ì±… ì›ë¬¸

> To reduce the dimensionality of a single point $\mathbf{x}_i$, we project it onto the top $k$ singular directions:

$$\mathbf{z}_i = \mathbf{x}_i V_k$$

- $V_k \in \mathbb{R}^{d \times k}$: ìƒìœ„ kê°œ íŠ¹ì´ë²¡í„°
- $\mathbf{z}_i \in \mathbb{R}^k$: ì¶•ì†Œëœ í‘œí˜„

#### ì˜ˆì‹œ: MNIST

```python
# ì›ë³¸: 784ì°¨ì› (28Ã—28)
# SVD í›„ ìƒìœ„ 50ê°œ â†’ 50ì°¨ì›
# ì••ì¶•ë¥ : 93%
# ìˆ«ì êµ¬ë³„: ì—¬ì „íˆ ê°€ëŠ¥!
```

---

## 1.4 Applications in Generative AI (ìƒì„± AI ì‘ìš©)

### 1.4.1 Diffusion Modelsì˜ í–‰ë ¬ ë³€í™˜

#### ì±… ì›ë¬¸

> In generative diffusion models, data points undergo a sequence of matrix transformations:

$$\mathbf{x}_{t+1} = A_t \mathbf{x}_t + \mathbf{b}_t, \quad t = 0, \ldots, T$$

- $\mathbf{x}_0$: ìˆœìˆ˜ ë…¸ì´ì¦ˆ (ì´ˆê¸° ì…ë ¥)
- $\mathbf{x}_T$: ìµœì¢… ìƒì„± ì´ë¯¸ì§€
- $A_t, \mathbf{b}_t$: $\mathbf{x}_t$ì™€ $t$ì˜ **ë¹„ì„ í˜• í•¨ìˆ˜** (ì‹ ê²½ë§ì´ í•™ìŠµ)

#### Whiteningê³¼ì˜ ì—°ê²°

| ë°©í–¥ | ê³¼ì • | Whitening ê´€ì  |
|------|------|----------------|
| Forward | ì´ë¯¸ì§€ â†’ ë…¸ì´ì¦ˆ | ë°ì´í„°ë¥¼ ì ì§„ì ìœ¼ë¡œ whitening |
| Reverse | ë…¸ì´ì¦ˆ â†’ ì´ë¯¸ì§€ | whitened â†’ ì›ë˜ êµ¬ì¡° ë³µì› |

---

### 1.4.2 Transformer ë©”ì»¤ë‹ˆì¦˜ (ìƒì„¸)

#### ì±… ì›ë¬¸

> The transformer architecture, introduced in 2017 by Vaswani et al. in "Attention is All You Need", revolutionized AI by leveraging self-attention mechanisms.

#### ì…ë ¥ í‘œí˜„

ì…ë ¥ ì‹œí€€ìŠ¤: $X = \{t_1, t_2, \ldots, t_n\} \in \mathbb{R}^{n \times d}$
- $n$: ì‹œí€€ìŠ¤ ê¸¸ì´
- $d$: ì„ë² ë”© ì°¨ì›
- $t_i \in \mathbb{R}^d$: ië²ˆì§¸ í† í°

#### Attention vs Self-Attention: ì§„í™” ê³¼ì •

Attentionì€ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ íŠ¹ì • ë¶€ë¶„ì— ë™ì ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤.

1. **Global Attention (2014, Bahdanau et al.)**:
   - ë‘ ê°œì˜ ë‹¤ë¥¸ ì‹œí€€ìŠ¤(ì˜ˆ: ì˜ì–´ ë¬¸ì¥, í”„ë‘ìŠ¤ì–´ ë¬¸ì¥) ê°„ì˜ ê´€ê³„ë¥¼ ë§¤í•‘.
   - ì¶œë ¥ ì‹œí€€ìŠ¤ì˜ íŠ¹ì • ì‹œì ì—ì„œ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ì–´ë–¤ í† í°ì´ ê°€ì¥ ì¤‘ìš”í•œì§€ ë™ì ìœ¼ë¡œ ê²°ì •.

2. **Self-Attention (2017, Vaswani et al.)**:
   - **ë‹¨ì¼ ì‹œí€€ìŠ¤ ë‚´**ì—ì„œ í† í° ê°„ì˜ ìƒí˜¸ì‘ìš©ì„ ê³„ì‚°.
   - ë¬¸ë§¥ì  ì˜ë¯¸(Contextual meaning)ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ ìê¸° ìì‹ ì˜ ë‹¤ë¥¸ ë¶€ë¶„ë“¤ì„ ì°¸ì¡°.

**ê²°ë¡ **: TransformerëŠ” 'Self-Attention' ë©”ì»¤ë‹ˆì¦˜ë§Œìœ¼ë¡œë„ ì¶©ë¶„íˆ ë³µì¡í•œ ë°ì´í„° êµ¬ì¡°ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆìŒì„ ì¦ëª…í•˜ë©° AIì˜ íŒ¨ëŸ¬ë‹¤ì„ì„ ë°”ê¿¨ìŠµë‹ˆë‹¤.

---

#### Q, K, V: ë™ì  ê°€ì¤‘ì¹˜ ë°°ë¶„ì˜ 3ìš”ì†Œ

Attentionì€ ê³ ì •ëœ í–‰ë ¬ ì—°ì‚°ì„ ë„˜ì–´, ì…ë ¥ ë°ì´í„°ì— ë”°ë¼ **ì—°ì‚°ì˜ ê°€ì¤‘ì¹˜ê°€ ì‹¤ì‹œê°„ìœ¼ë¡œ ë³€í•˜ëŠ”(Data-dependent weight)** ì„ í˜• ë³€í™˜ì˜ í™•ì¥íŒì…ë‹ˆë‹¤.

- **Query ($Q$)**: í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ í† í°ì´ "ë‹¤ë¥¸ í† í°ë“¤ì—ê²Œ ë¬»ëŠ” ì§ˆë¬¸" ë˜ëŠ” "ì°¾ê³ ì í•˜ëŠ” íŠ¹ì§•".
- **Key ($K$)**: ê° í† í°ì´ ê°€ì§„ "ì •ë³´ì˜ ì§€í‘œ" ë˜ëŠ” "ë§¤ì¹­ì„ ìœ„í•œ ë¼ë²¨".
- **Value ($V$)**: ì‹¤ì œ ì „ë‹¬í•˜ê³ ì í•˜ëŠ” "ì •ë³´ì˜ ì‹¤ì²´" ë˜ëŠ” "ì½˜í…ì¸ ".

**ë™ì‘ ì›ë¦¬: Dual Space Projection**
1. **$Q \cdot K^\top$**: Query ê³µê°„ê³¼ Key ê³µê°„ ì‚¬ì´ì˜ ë‚´ì ì„ í†µí•´, ë‘ ì •ë³´ê°€ ì–¼ë§ˆë‚˜ ì •ë ¬(Align)ë˜ì–´ ìˆëŠ”ì§€ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
2. **Softmax**: ê³„ì‚°ëœ ìœ ì‚¬ë„ë¥¼ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜í•˜ì—¬ ì—ë„ˆì§€ë¥¼ ë°°ë¶„í•©ë‹ˆë‹¤.
3. **$\alpha \cdot V$**: ë°°ë¶„ëœ ê°€ì¤‘ì¹˜ì— ë”°ë¼ ìµœì¢… ì •ë³´ë¥¼ ì·¨í•©í•˜ì—¬ ë‹¤ìŒ ì¸µìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.

**ì™œ ì—­í• ì„ ë¶„ë¦¬í•˜ëŠ”ê°€? (Role Specialization)**
ë§Œì•½ $Q=K=V=X$ë¼ë©´, ëª¨ë“  í† í°ì€ í•­ìƒ ìê¸° ìì‹ ì—ê²Œë§Œ ê°•í•˜ê²Œ ë°˜ì‘í•˜ê²Œ ë˜ì–´ ë¬¸ë§¥ì„ íŒŒì•…í•˜ëŠ” ìœ ì—°ì„±ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤. ê°ê¸° ë‹¤ë¥¸ í•™ìŠµ ê°€ëŠ¥ ê°€ì¤‘ì¹˜ í–‰ë ¬($W_Q, W_K, W_V$)ì„ í†µí•´ í† í°ì€ **"ë¬´ì—‡ì„ ì°¾ì„ì§€"**ì™€ **"ë¬´ì—‡ì„ ì œê³µí• ì§€"**ë¥¼ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.

---

#### í•˜ë“œì›¨ì–´ ìˆ˜ì¤€ì—ì„œ ì •í•´ì§„ ì—­í•  ë¶„í™”

"W_QëŠ” ì°¾ëŠ” ì—­í• , W_KëŠ” ì°¾ì•„ì§€ëŠ” ì—­í• "ì´ë¼ê³  ëª…ì‹œì ìœ¼ë¡œ ê°€ë¥´ì¹˜ì§€ ì•Šì•„ë„ ì—­ì „íŒŒ(Backpropagation) ê³¼ì •ì—ì„œ ì—­í• ì´ ë¶„í™”ë©ë‹ˆë‹¤.

1. **ê³„ì‚° êµ¬ì¡°ì˜ ë¹„ëŒ€ì¹­ì„±**: $Q \cdot K^\top$ ì—°ì‚°ì—ì„œ $Q$ëŠ” í–‰ë ¬ ê³±ì˜ ì•ë¶€ë¶„ì—, $K$ëŠ” ë’·ë¶€ë¶„ì— ìœ„ì¹˜í•©ë‹ˆë‹¤. ì´ ìœ„ì¹˜ì˜ ì°¨ì´ê°€ Gradientê°€ íë¥´ëŠ” ê²½ë¡œë¥¼ ë‹¤ë¥´ê²Œ ë§Œë“­ë‹ˆë‹¤.
2. **Gradient Paths**:
   - $W_Q$ëŠ” "ì–´ë–¤ $K$ë¥¼ ì„ íƒí–ˆì„ ë•Œ Lossê°€ ì¤„ì–´ë“œëŠ”ê°€"ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤ (**Searching strategy**).
   - $W_K$ëŠ” "ì–´ë–¤ $Q$ì— ì˜í•´ ì„ íƒë˜ì—ˆì„ ë•Œ Lossê°€ ì¤„ì–´ë“œëŠ”ê°€"ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤ (**Representing strategy**).
   - $W_V$ëŠ” ìµœì¢… ì¶œë ¥ê°’ì— ì§ì ‘ ê³±í•´ì§€ë¯€ë¡œ, "ì–´ë–¤ ì •ë³´ë¥¼ ì „ë‹¬í•´ì•¼ Lossê°€ ì¤„ì–´ë“œëŠ”ê°€"ì— ì§‘ì¤‘í•©ë‹ˆë‹¤ (**Content optimization**).

**ê²°ë¡ **: ìˆ˜ì¹˜ ìµœì í™” ê³¼ì •ì—ì„œ ê° ê°€ì¤‘ì¹˜ í–‰ë ¬ì€ ìˆ˜ë ´(Convergence)ì„ ìœ„í•´ ìì—°ìŠ¤ëŸ½ê²Œ ì„œë¡œ ë‹¤ë¥¸ ìˆ˜í•™ì  ì—­í• ì„ í•˜ë„ë¡ ì „ë¬¸í™”ë©ë‹ˆë‹¤.

---

#### Gradient ì—…ë°ì´íŠ¸ ìˆ˜ì‹ì˜ ê° í•­ ì„¤ëª…

ìœ„ì—ì„œ `W_K -= lr Ã— (... Ã— Q)` ê°™ì€ í‘œê¸°ë¥¼ ì¼ëŠ”ë°, ê° í•­ì˜ ì˜ë¯¸:

**`lr` = Learning Rate (í•™ìŠµë¥ )**

```
lr = 0.001  (ì˜ˆì‹œ)

ì—­í• : gradient ë°©í–¥ìœ¼ë¡œ ì–¼ë§ˆë‚˜ í¬ê²Œ ì´ë™í• ì§€ ê²°ì •í•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°
- ì‘ì„ìˆ˜ë¡: ì²œì²œíˆ, ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµ
- í´ìˆ˜ë¡: ë¹ ë¥´ì§€ë§Œ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ
```

**`...` = ìƒëµëœ Chain Rule í•­ë“¤**

Lossì—ì„œ scoreê¹Œì§€ ì—­ì „íŒŒë˜ëŠ” ì¤‘ê°„ gradientë“¤:

**ì „ì²´ chain rule**:

$$\frac{\partial L}{\partial W_K} = \underbrace{\frac{\partial L}{\partial \text{output}} \times \frac{\partial \text{output}}{\partial \alpha} \times \frac{\partial \alpha}{\partial \text{score}}}_{\text{ì´ ë¶€ë¶„ì´ "..."}} \times \frac{\partial \text{score}}{\partial K} \times \frac{\partial K}{\partial W_K}$$

**êµ¬ì²´ì ìœ¼ë¡œ í¼ì¹˜ë©´**:
- $\text{score} = Q \cdot K^\top / \sqrt{d}$
- $\alpha = \text{softmax}(\text{score})$ â† $\frac{\partial \alpha}{\partial \text{score}} = \alpha(1-\alpha)$ í˜•íƒœ
- $\text{output} = \alpha \cdot V$
- $L = f(\text{output}, \text{target})$ â† $\frac{\partial L}{\partial \text{output}} = (\text{ì˜ˆì¸¡} - \text{ì •ë‹µ})$ í˜•íƒœ

**ì™œ `... Ã— Q` í˜•íƒœê°€ ë˜ë‚˜?**

$\frac{\partial \text{score}}{\partial K} = Q$ â† $\text{score} = Q \cdot K^\top$ ë‹ˆê¹Œ

$$\frac{\partial L}{\partial K} = \left(\frac{\partial L}{\partial \text{output}} \times \frac{\partial \text{output}}{\partial \alpha} \times \frac{\partial \alpha}{\partial \text{score}}\right) \times Q = (\ldots) \times Q$$

â†’ $Q$ê°€ gradientì˜ "ë°©í–¥"ì„ ê²°ì •í•œë‹¤!

**ì‹¤ì œ ì½”ë“œì—ì„œì˜ í˜•íƒœ**:

```python
# PyTorchì—ì„œëŠ” ìë™ ë¯¸ë¶„ì´ ì²˜ë¦¬í•˜ì§€ë§Œ, í’€ì–´ì“°ë©´:
dK = upstream_gradient @ Q  # Kì˜ gradientëŠ” Qì— ì˜ì¡´
W_K = W_K - lr * (X.T @ dK)
```

**í•µì‹¬ í¬ì¸íŠ¸**:
- `lr`: ì‚¬ëŒì´ ì •í•˜ëŠ” ìƒìˆ˜ (í•™ìŠµ ì†ë„)
- `...`: softmax, lossë¥¼ ê±°ì³ ìë™ ê³„ì‚°ë˜ëŠ” gradient í•­ë“¤
- ì¤‘ìš”í•œ ê±´ **ì–´ë–¤ ê°’ì´ ê³±í•´ì§€ëŠ”ê°€** â†’ ê·¸ê²ƒì´ ì—…ë°ì´íŠ¸ ë°©í–¥ì„ ê²°ì •

---

#### Self-Attention (ì±… ì›ë¬¸ ìˆ˜ì‹)

**Step 1**: Query, Key, Value ê³„ì‚°
$$Q = \hat{t}_{n+1}^{(k)} W_Q, \quad K = XW_K, \quad V = XW_V$$

**Step 2**: Attention ê°€ì¤‘ì¹˜
$$\alpha_i^{(k)} = \text{softmax}\left(\frac{Q \cdot K^\top}{\sqrt{d}}\right)_i = \frac{\exp\left(\frac{Q K_{i,:}^\top}{\sqrt{d}}\right)}{\sum_{j=1}^n \exp\left(\frac{Q K_{j,:}^\top}{\sqrt{d}}\right)}$$

**Step 3**: ê°€ì¤‘ í•©ì‚°
$$\mathbf{z}^{(k)} = \sum_{i=1}^n \alpha_i^{(k)} V_i$$

#### ì™œ $\sqrt{d}$ë¡œ ë‚˜ëˆ„ëŠ”ê°€?

**ë¬¸ì œ ìƒí™©**:

ë‚´ì : $q \cdot k = q_1 k_1 + q_2 k_2 + \cdots + q_d k_d$

$q, k$ê°€ í‰ê·  0, ë¶„ì‚° 1ì¸ ëœë¤ ë²¡í„°ë¼ê³  ê°€ì •í•˜ë©´:
- ê° í•­ $q_i k_i$ì˜ ë¶„ì‚° $\approx 1$
- $d$ê°œ í•­ì˜ í•©ì˜ ë¶„ì‚° $\approx d$ (ë…ë¦½ì´ë¯€ë¡œ í•©ì‚°)

$d = 512$ë©´?
- ë‚´ì ê°’ì´ $-30 \sim +30$ ë²”ìœ„ë¡œ ì»¤ì§
- $\text{softmax}(30) \approx 1.0$, $\text{softmax}(-30) \approx 0.0$
- â†’ ê±°ì˜ one-hot! (í•˜ë‚˜ë§Œ 1, ë‚˜ë¨¸ì§€ 0)
- â†’ gradientê°€ ê±°ì˜ 0 (í•™ìŠµ ì•ˆ ë¨!)

**í•´ê²°: $\sqrt{d}$ë¡œ ë‚˜ëˆ„ê¸°**

$$\text{Var}\left(\frac{q \cdot k}{\sqrt{d}}\right) = \frac{d}{d} = 1$$

ì´ì œ softmax ì…ë ¥ì´ ì ë‹¹í•œ ë²”ìœ„ $(-3 \sim +3)$
â†’ ë¶€ë“œëŸ¬ìš´ í™•ë¥  ë¶„í¬ â†’ gradientê°€ ì˜ íë¦„ â†’ í•™ìŠµ ê°€ëŠ¥!

**ì‹œê°í™”**:

| $\sqrt{d}$ ì—†ì´ | $\sqrt{d}$ë¡œ ë‚˜ëˆ„ë©´ |
|----------------|-------------------|
| $\text{softmax}([5, 30, -20])$ | $\text{softmax}([0.5, 1.2, -0.8])$ |
| $= [0.00, 1.00, 0.00]$ | $= [0.25, 0.50, 0.25]$ |
| â†‘ ê·¹ë‹¨ì ! | â†‘ ë¶€ë“œëŸ¬ì›€! |

#### ë¹„ì„ í˜• ë³€í™˜ + ì •ê·œí™” (ì±… ì›ë¬¸)

$$\hat{t}_{n+1}^{(k+1)} = \text{LayerNorm}\left(\sigma\left(W_2 \sigma(W_1 \mathbf{z}^{(k)}) + \mathbf{b}\right)\right)$$

**Layer Normalization**:
$$\text{LayerNorm}(\mathbf{x}) = \frac{\mathbf{x} - \mu}{\sqrt{\sigma^2 + \epsilon}} \odot \bm{\gamma} + \bm{\beta}$$

- $\mu = \frac{1}{d}\sum_{i=1}^d x_i$: í‰ê· 
- $\sigma^2 = \frac{1}{d}\sum_{i=1}^d (x_i - \mu)^2$: ë¶„ì‚°
- $\bm{\gamma}, \bm{\beta}$: í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°

---

#### Layer Norm vs Batch Norm: ì™œ TransformerëŠ” LayerNormì„ ì“°ë‚˜?

**ë¨¼ì €, ì™œ ì •ê·œí™”ê°€ í•„ìš”í•œê°€?**

ì‹ ê²½ë§ì´ ê¹Šì–´ì§€ë©´ ìƒê¸°ëŠ” ë¬¸ì œ:

| ì¸µ | ì¶œë ¥ ë¶„í¬ | ë¬¸ì œ |
|----|----------|------|
| ì¸µ 1 | í‰ê·  0, ë¶„ì‚° 1 | ì •ìƒ |
| ì¸µ 10 | í‰ê·  5, ë¶„ì‚° 100 | ìŠ¤ì¼€ì¼ ë³€í•¨ |
| ì¸µ 50 | í‰ê·  ???, ë¶„ì‚° ??? | í­ë°œ ë˜ëŠ” ì†Œë©¸ |

â†’ í•™ìŠµ ë¶ˆì•ˆì •, gradient vanishing/exploding
â†’ í•´ê²°ì±…: ê° ì¸µì˜ ì¶œë ¥ì„ ì •ê·œí™”!

**Batch Normalization (2015)** - CNNì˜ í‘œì¤€

ë°ì´í„° í˜•íƒœ: $(\text{Batch}, \text{Feature})$

| | Feature 1 | Feature 2 | Feature 3 |
|---|---|---|---|
| img1 | $x_{11}$ | $x_{12}$ | $x_{13}$ |
| img2 | $x_{21}$ | $x_{22}$ | $x_{23}$ |
| img3 | $x_{31}$ | $x_{32}$ | $x_{33}$ |
| img4 | $x_{41}$ | $x_{42}$ | $x_{43}$ |
| | â†“ | â†“ | â†“ |
| | $\mu_1, \sigma_1^2$ | $\mu_2, \sigma_2^2$ | $\mu_3, \sigma_3^2$ |

**"ê°™ì€ featureì— ëŒ€í•´ ë°°ì¹˜ ë‚´ ëª¨ë“  ìƒ˜í”Œì˜ í‰ê· /ë¶„ì‚°"** (ì„¸ë¡œ)

**Layer Normalization (2016)** - Transformerì˜ í‘œì¤€

ë°ì´í„° í˜•íƒœ: $(\text{Batch}, \text{Feature})$

| | dim1 | dim2 | dim3 | ì •ê·œí™” |
|---|---|---|---|---|
| word1 | $x_{11}$ | $x_{12}$ | $x_{13}$ | â† $\mu_1, \sigma_1^2$ |
| word2 | $x_{21}$ | $x_{22}$ | $x_{23}$ | â† $\mu_2, \sigma_2^2$ |
| word3 | $x_{31}$ | $x_{32}$ | $x_{33}$ | â† $\mu_3, \sigma_3^2$ |
| word4 | $x_{41}$ | $x_{42}$ | $x_{43}$ | â† $\mu_4, \sigma_4^2$ |

**"ê° ìƒ˜í”Œ ë‚´ì—ì„œ ëª¨ë“  featureì˜ í‰ê· /ë¶„ì‚°"** (ê°€ë¡œ)

**ì‹œê°ì  ë¹„êµ**:

ì…ë ¥ í…ì„œ: $(\text{Batch}=4, \text{Features}=3)$

| | Feature 1 | Feature 2 | Feature 3 |
|---|---|---|---|
| Sample 1 | 0.5 | 1.2 | -0.3 |
| Sample 2 | 0.8 | 0.9 | 0.1 |
| Sample 3 | -0.2 | 1.5 | 0.7 |
| Sample 4 | 0.3 | 0.6 | -0.1 |

- **BatchNorm**: â†“ ì„¸ë¡œë¡œ ì •ê·œí™” (ê°™ì€ feature, ë‹¤ë¥¸ ìƒ˜í”Œ)
  - Feature 1ì˜ $\mu, \sigma^2$ ê³„ì‚°: $(0.5, 0.8, -0.2, 0.3)$

- **LayerNorm**: â†’ ê°€ë¡œë¡œ ì •ê·œí™” (ê°™ì€ ìƒ˜í”Œ, ë‹¤ë¥¸ feature)
  - Sample 1ì˜ $\mu, \sigma^2$ ê³„ì‚°: $(0.5, 1.2, -0.3)$

**ì™œ TransformerëŠ” LayerNorm?**

| ë¬¸ì œ | BatchNorm | LayerNorm |
|------|-----------|-----------|
| **ê°€ë³€ ì‹œí€€ìŠ¤ ê¸¸ì´** | âŒ ë¬¸ì œ | âœ… OK |
| **ì‘ì€ ë°°ì¹˜ í¬ê¸°** | âŒ í†µê³„ ë¶ˆì•ˆì • | âœ… OK |
| **ì¶”ë¡  ì‹œ ë‹¨ì¼ ìƒ˜í”Œ** | âŒ running mean í•„ìš” | âœ… ê·¸ëŒ€ë¡œ ì‚¬ìš© |
| **ì‹œí€€ìŠ¤ ë‚´ ìœ„ì¹˜ ë¬´ê´€** | âŒ ìœ„ì¹˜ë³„ë¡œ ë‹¤ë¦„ | âœ… ë™ì¼í•˜ê²Œ ì²˜ë¦¬ |

**ìƒì„¸ ì„¤ëª…**:

**ë¬¸ì œ 1: ê°€ë³€ ì‹œí€€ìŠ¤ ê¸¸ì´**
- **BatchNorm**: ë°°ì¹˜ ë‚´ ë¬¸ì¥ë“¤ "I love you" (3ë‹¨ì–´), "Hello" (1ë‹¨ì–´), "How are you doing" (4ë‹¨ì–´)
  - ê°™ì€ "ìœ„ì¹˜"ë¼ë¦¬ í‰ê· ? ìœ„ì¹˜ 3ì—ëŠ” ì¼ë¶€ ë¬¸ì¥ë§Œ ìˆìŒ! â†’ padding ì²˜ë¦¬ ë³µì¡
- **LayerNorm**: ê° í† í°ì„ ë…ë¦½ì ìœ¼ë¡œ ì •ê·œí™” â†’ ì‹œí€€ìŠ¤ ê¸¸ì´ ìƒê´€ì—†ìŒ!

**ë¬¸ì œ 2: ë°°ì¹˜ í¬ê¸°**
- **BatchNorm**: ë°°ì¹˜ í¬ê¸° $= 2$ë©´? â†’ 2ê°œ ìƒ˜í”Œë¡œ $\mu, \sigma^2$ ì¶”ì • = ë§¤ìš° ë¶ˆì•ˆì •
- **LayerNorm**: feature ì°¨ì›ì´ 512ë©´? â†’ 512ê°œ ê°’ìœ¼ë¡œ $\mu, \sigma^2$ ì¶”ì • = ì¶©ë¶„íˆ ì•ˆì •

**ë¬¸ì œ 3: ì¶”ë¡  ì‹œ (inference)**
- **BatchNorm**: í•™ìŠµ ì‹œ ë°°ì¹˜ í†µê³„, ì¶”ë¡  ì‹œ running mean/var ì‚¬ìš© â†’ í•™ìŠµê³¼ ì¶”ë¡ ì´ ë‹¤ë¥´ê²Œ ë™ì‘
- **LayerNorm**: í•™ìŠµ = ì¶”ë¡  (ë™ì¼í•˜ê²Œ ë™ì‘) â†’ ë” ì˜ˆì¸¡ ê°€ëŠ¥í•˜ê³  ì•ˆì •ì 

**í•œ ì¤„ ìš”ì•½**:
- **BatchNorm**: "ë‹¤ë¥¸ ìƒ˜í”Œë“¤ê³¼ ë¹„êµí•´ì„œ ì •ê·œí™”" â†’ ë°°ì¹˜ê°€ ì»¤ì•¼ í•˜ê³ , ê³ ì • ê¸¸ì´ê°€ ì¢‹ìŒ (CNN)
- **LayerNorm**: "ìê¸° ìì‹  ë‚´ì—ì„œ ì •ê·œí™”" â†’ ë°°ì¹˜/ê¸¸ì´ ìƒê´€ì—†ìŒ (Transformer, RNN)

**ì½”ë“œë¡œ ë¹„êµ**:

```python
import torch
import torch.nn as nn

x = torch.randn(4, 512)  # batch=4, features=512

# BatchNorm: feature ì°¨ì›(512) ì§€ì •
bn = nn.BatchNorm1d(512)
out_bn = bn(x)  # ê° featureë³„ë¡œ batch í‰ê· /ë¶„ì‚°

# LayerNorm: feature ì°¨ì›(512) ì§€ì •
ln = nn.LayerNorm(512)
out_ln = ln(x)  # ê° ìƒ˜í”Œë³„ë¡œ feature í‰ê· /ë¶„ì‚°

# í™•ì¸
print(out_bn[0].mean(), out_bn[0].std())  # 0ì´ ì•„ë‹˜ (ìƒ˜í”Œë³„ ì•„ë‹˜)
print(out_ln[0].mean(), out_ln[0].std())  # â‰ˆ 0, â‰ˆ 1 (ìƒ˜í”Œ ë‚´ ì •ê·œí™”)
```

#### Multi-Head Attention

$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_H) W^O$$

ê° headê°€ ì„œë¡œ ë‹¤ë¥¸ ê´€ê³„ íŒ¨í„´ í•™ìŠµ:
- head 1: ë¬¸ë²•ì  ê´€ê³„ (ì£¼ì–´-ë™ì‚¬)
- head 2: ì˜ë¯¸ì  ê´€ê³„ (ê³ ì–‘ì´-ê·€ì—½ë‹¤)
- head 3: ì§€ì‹œì–´ ê´€ê³„ ("it" â†’ "cat")

#### ìµœì¢… í† í° ì˜ˆì¸¡

$$p(t_{n+1}|X) = \text{softmax}(W_{\text{out}} \hat{t}_{n+1}(X) + \mathbf{b}_{\text{out}})$$

---

## Exercises (ì—°ìŠµë¬¸ì œ)

### Exercise 1.1.1: Matrix Multiplication and Elliptical Dynamics

> **ë¬¸ì œ**: ì´ˆê¸° ë²¡í„° $\mathbf{x}_0 = (a, 0)^\top$ì— í–‰ë ¬ $A$ë¥¼ ë°˜ë³µ ì ìš©í•˜ì—¬ íƒ€ì› ìœ„ì˜ ì ë“¤ì„ ìƒì„±í•˜ëŠ” $A$ ì„¤ê³„í•˜ê¸°.

**(a)** ë°˜ì¥ì¶• ë¹„ìœ¨ $a/b$ì¸ íƒ€ì› ìœ„ì— $\mathbf{x}_t = A^t \mathbf{x}_0$ ë†“ê¸°

**í’€ì´**:
íšŒì „ í–‰ë ¬ì„ ìŠ¤ì¼€ì¼ë§ê³¼ ê²°í•©:
$$A = \begin{pmatrix} a & 0 \\ 0 & b \end{pmatrix} \begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix} \begin{pmatrix} 1/a & 0 \\ 0 & 1/b \end{pmatrix}$$

**(b)** $t \to \infty$ì¼ ë•Œ íƒ€ì› ì „ì²´ë¥¼ ë®ìœ¼ë ¤ë©´?

**ë‹µ**: $\theta$ê°€ $\pi$ì˜ **ë¬´ë¦¬ìˆ˜ ë°°**ì—¬ì•¼ í•¨
- ìœ ë¦¬ìˆ˜ ë°°: ì£¼ê¸°ì  â†’ ìœ í•œ ê°œ ì 
- ë¬´ë¦¬ìˆ˜ ë°°: ë¹„ì£¼ê¸°ì  â†’ ì¡°ë°€í•˜ê²Œ ì±„ì›€

---

### Exercise 1.1.3: Einstein Summation (KAIST Challenge 1)

**(1)** $\mathbf{y} = A\mathbf{x}$:
$$y_i = A_{ij} x_j$$

**(2)** $\|A\|_F^2$:
$$\|A\|_F^2 = A_{ij} A_{ij}$$

**NumPy êµ¬í˜„**:
```python
import numpy as np

# (1) í–‰ë ¬-ë²¡í„° ê³±
y = np.einsum('ij,j->i', A, x)

# (2) Frobenius ë…¸ë¦„
frob_sq = np.einsum('ij,ij->', A, A)
```

---

### Exercise 1.2.2: SVD for Matrix Completion (KAIST Challenge 2)

> **ë¬¸ì œ**: 90% ì—ë„ˆì§€ë¥¼ ìœ ì§€í•˜ëŠ” ìµœì†Œ k ì°¾ê¸°

**Python ì½”ë“œ**:
```python
import numpy as np

def find_k_for_energy(A, threshold=0.90):
    U, s, Vt = np.linalg.svd(A, full_matrices=False)

    total_energy = np.sum(s**2)
    cumulative = np.cumsum(s**2) / total_energy

    k = np.searchsorted(cumulative, threshold) + 1
    return k, cumulative

# ì˜ˆì‹œ
A = np.random.randn(100, 50)
k, cumsum = find_k_for_energy(A)
print(f"90% ì—ë„ˆì§€ ìœ ì§€: k = {k}")
```

---

## Data Whitening - VAE/Diffusion ì—°ê²°

### ğŸ’¡ ë°ì´í„° ê¸°ë°€í™”(Whitening)ì˜ ëª©ì : Isotropic Normalization

í˜„ì‹¤ì˜ ë°ì´í„°ëŠ” ë³€ìˆ˜ ê°„ì˜ ë†’ì€ ìƒê´€ê´€ê³„ë¡œ ì¸í•´ íŠ¹ì • ë°©í–¥ìœ¼ë¡œ ì¹˜ìš°ì¹œ **ì´ë°©ì„±(Anisotropic) ë¶„í¬**ë¥¼ ê°€ì§‘ë‹ˆë‹¤. Whiteningì€ ì´ë¥¼ ëª¨ë“  ë°©í–¥ìœ¼ë¡œ ê· ì¼í•˜ê²Œ í¼ì§„ **ë“±ë°©ì„±(Isotropic) ë¶„í¬**ë¡œ ë³€í™˜í•˜ì—¬ ë°ì´í„°ë¥¼ 'í‘œì¤€í™”'í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.

-   **ë³€í™˜ ì „**: íŠ¹ì • ì¶•ì— ì •ë³´ê°€ ì ë ¤ ìˆê³  ë³€ìˆ˜ ê°„ ê°„ì„­(Correlation)ì´ ì‹¬í•´ ëª¨ë¸ì´ í•™ìŠµí•˜ê¸° ê¹Œë‹¤ë¡œì›€.
-   **ë³€í™˜ í›„**: ëª¨ë“  ì¶•ì´ í†µê³„ì ìœ¼ë¡œ ë…ë¦½ì ì´ë©° ë¶„ì‚°ì´ 1ì¸ ìƒíƒœ. ì•Œê³ ë¦¬ì¦˜ì´ íŠ¹ì • ë°©í–¥ì˜ í¸í–¥ ì—†ì´ ê° ì„±ë¶„ì˜ ë³¸ì§ˆì  íŠ¹ì§•ì„ íŒŒì•…í•˜ê¸° ìµœì í™”ëœ ìƒíƒœê°€ ë©ë‹ˆë‹¤.

### ğŸ“ ìˆ˜ì‹ì˜ ì˜ë¯¸

ë°ì´í„°ì˜ ëª¨ì–‘ ì§€ë„ì¸ **ê³µë¶„ì‚° í–‰ë ¬(Covariance Matrix)**ì„ í†µí•´ íƒ€ì›ì„ í…ë‹ˆë‹¤.

1.  **ê³µë¶„ì‚° ê³„ì‚°**: $C = \frac{1}{n}X^\top X = V\Sigma^2 V^\top$
    -   $V$: íƒ€ì›ì´ ê¸°ìš¸ì–´ì§„ **ë°©í–¥** (íšŒì „ì¶•)
    -   $\Sigma^2$: ê·¸ ë°©í–¥ìœ¼ë¡œ **ì–¼ë§ˆë‚˜ ì°Œê·¸ëŸ¬ì ¸ ìˆëŠ”ì§€** (ë¶„ì‚°)
1.  **ê³µë¶„ì‚° ë¶„í•´**: $C = V \Sigma^2 V^\top$ (**ë¶„ì‚°** ìŠ¤ì¼€ì¼)
2.  **Whitening ë³€í™˜**: $X_{\text{white}} = X V \Sigma^{-1}$

#### âœï¸ ì™œ $\Sigma^{-1}$ì¸ê°€? (ìˆ˜í•™ì  ì§ê´€)

ê³µë¶„ì‚° $C$ê°€ $\Sigma^2$(ë¶„ì‚°)ì˜ ìŠ¤ì¼€ì¼ì„ ê°€ì§€ê³  ìˆë‹¤ëŠ” ì ì´ í•µì‹¬ì…ë‹ˆë‹¤.
- ë°ì´í„°ë¥¼ ì´ ì¶•ìœ¼ë¡œ íˆ¬ì˜($XV$)í•˜ë©´ ê·¸ ë³€ë™ì„±ì˜ í¬ê¸°ëŠ” $\Sigma$(**í‘œì¤€í¸ì°¨**)ë§Œí¼ì…ë‹ˆë‹¤.
- ì´ ë°ì´í„°ë¥¼ ëª¨ë“  ë°©í–¥ìœ¼ë¡œ í¬ê¸°ê°€ 1ì¸ 'ë‹¨ìœ„ ì›'ìœ¼ë¡œ ë§Œë“¤ë ¤ë©´, ê° ì¶•ì˜ ê¸¸ì´ë¥¼ ê·¸ ì¶•ì˜ í‘œì¤€í¸ì°¨ì¸ $\Sigma$ë¡œ ë‚˜ëˆ„ì–´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.
- ë”°ë¼ì„œ $\Sigma^{-1}$ì„ ê³±í•˜ëŠ” ê²ƒì€ **"ê° ì¶•ì˜ í‘œì¤€í¸ì°¨ë¡œ ë‚˜ëˆ„ì–´ ë¶„ì‚°ì„ 1ë¡œ ë§ì¶˜ë‹¤"**ëŠ” ì˜ë¯¸ê°€ ë©ë‹ˆë‹¤.

> **ê²°ê³¼**: $(X_{\text{white}})^\top (X_{\text{white}}) = I$. ëª¨ë“  ê³µë¶„ì‚°ì´ ì‚¬ë¼ì§€ê³  ë¶„ì‚°ì´ 1ì¸ ì™„ë²½í•œ ì •ì›í˜• ë¶„í¬ê°€ ë©ë‹ˆë‹¤.
    -   $V$ ê³±í•˜ê¸°: íƒ€ì›ì„ ë˜‘ë°”ë¡œ ì„¸ìš°ê¸° (**íšŒì „**)
    -   $\Sigma^{-1}$ ê³±í•˜ê¸°: ê¸¸ê²Œ ëŠ˜ì–´ì§„ ì¶•ì„ 1ë¡œ ì••ì¶•í•˜ê¸° (**ìŠ¤ì¼€ì¼ë§**)

### ğŸ¨ ìƒì„± AIì—ì„œì˜ ê²°ì •ì  ì—­í• 

| ëª¨ë¸ | Whitening ê´€ì ì˜ ì—­í•  | ì™œ í•„ìš”í•œê°€? |
| :--- | :--- | :--- |
| **VAE** | ì ì¬ ê³µê°„ì„ **'í‘œì¤€ì ì¸ ê³µ($\mathcal{N}(0,I)$)'**ìœ¼ë¡œ ê°•ì œ | ë°ì´í„°ê°€ ì¤‘êµ¬ë‚œë°©ìœ¼ë¡œ ì €ì¥ë˜ì§€ ì•Šê²Œ í•˜ì—¬, ê³µ ì•ˆì˜ ì•„ë¬´ ì ì´ë‚˜ ì°ì–´ë„(Sampling) ìƒˆë¡œìš´ ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ê²Œ í•¨. |
| **Diffusion** | ì´ë¯¸ì§€ë¥¼ ìˆœìˆ˜ ë…¸ì´ì¦ˆ(**'ì™„ë²½í•œ ê³µ'**)ë¡œ ë…¹ì„ | ë°ì´í„°ë¥¼ ê·¹í•œìœ¼ë¡œ Whitening í•˜ì—¬ ì •ë³´ë¥¼ ì™„ì „íˆ ì§€ìš´ ë’¤, ì—­ê³¼ì •(Reverse)ì„ í†µí•´ ë‹¤ì‹œ íƒ€ì›ì˜ êµ¬ì¡°(ì´ë¯¸ì§€)ë¥¼ ìŒ“ì•„ ì˜¬ë¦¬ëŠ” ë§ˆë²•ì„ ë¶€ë¦¼. |
| **Normalizing Flow** | ë³µì¡í•œ ë¶„í¬ $\to$ ê°€ìš°ì‹œì•ˆ ë³€í™˜ | ê°€ì—­ í–‰ë ¬(Invertible Matrix)ì„ ì—°ì†ìœ¼ë¡œ ê³±í•´ ë³µì¡í•œ ë°ì´í„°ë¥¼ ë‹¨ìˆœí•œ ê³µ ëª¨ì–‘ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í™•ë¥  ê³„ì‚°ì„ ì‰½ê²Œ í•¨. |

> **í•µì‹¬**: Whiteningì€ ë¬´ì§ˆì„œí•œ ë°ì´í„°ë¥¼ AIê°€ ìš”ë¦¬í•˜ê¸° ê°€ì¥ ì¢‹ì€ 'ê¹¨ë—í•œ ì›í˜• ìƒíƒœ'ë¡œ ì´ˆê¸°í™”í•˜ëŠ” í•„ìˆ˜ ì „ì²˜ë¦¬ ê³¼ì •ì…ë‹ˆë‹¤.

---

## ë…¸íŠ¸ë¶ ê°€ì´ë“œ

### [MNIST-SVD.ipynb](LinearAlgebra/MNIST-SVD.ipynb)

**ë°°ìš°ëŠ” ê²ƒ**:
- ì´ë¯¸ì§€ ë°°ì¹˜ë¥¼ í–‰ë ¬ë¡œ ë³€í™˜ (100Ã—784)
- í‰ê·  ì´ë¯¸ì§€ì™€ "eigendigit" ì‹œê°í™”
- Rank-k ì¬êµ¬ì„±ìœ¼ë¡œ ì••ì¶• íš¨ê³¼ í™•ì¸

**í•µì‹¬ ì½”ë“œ**:
```python
X_centered = X - mean_image
U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)

# Rank-k ì¬êµ¬ì„±
x_reconstructed = mean_image + (x @ Vt[:k].T) @ Vt[:k]
```

### [Ex1-1-1image.ipynb](LinearAlgebra/Ex1-1-1image.ipynb)

**ë°°ìš°ëŠ” ê²ƒ**:
- RGB ì´ë¯¸ì§€: $T \in \mathbb{R}^{H \times W \times 3}$
- ì±„ë„ ì¶”ì¶œ, í‰ê·  ê°•ë„ ê³„ì‚°

---

## ë‹¤ìŒ ì¥ê³¼ì˜ ì—°ê²°

| ì—°ê²° | ì±•í„° | ê°œë… |
|------|------|------|
| â†’ Ch.2 | Calculus | Jacobian, Chain Rule (KAIST Ch.3) |
| â†’ Ch.3 | Optimization | Gradient Descent, Adam (KAIST Ch.5-6) |
| â†’ Ch.4 | Neural Networks | CNN, ResNet (KAIST Ch.7-8) |
| â†’ Ch.5 | Probability | ê³µë¶„ì‚°, ê°€ìš°ì‹œì•ˆ ë¶„í¬ |
| â†’ Ch.6 | Information | KL Divergence, VAE |
| â†’ Ch.7 | Stochastic | Diffusion, Brownian Motion |

---

## í•µì‹¬ ìš”ì•½

| ê°œë… | ì •ì˜ | AI ì‘ìš© |
|------|------|---------|
| **Vector** | ìˆœì„œ ìˆëŠ” ìˆ«ì ëª¨ìŒ | ë°ì´í„° í‘œí˜„ |
| **Matrix** | ì„ í˜• ë³€í™˜ì˜ í‘œí˜„ | ê°€ì¤‘ì¹˜, íšŒì „, íˆ¬ì˜ |
| **Tensor** | ë‹¤ì°¨ì› ë°°ì—´ | ì´ë¯¸ì§€, ë¹„ë””ì˜¤, ë°°ì¹˜ |
| **Einstein** | ë°˜ë³µ ì¸ë±ìŠ¤ = í•©ì‚° | GPU í…ì„œ ì—°ì‚° |
| **Convolution** | ìŠ¬ë¼ì´ë”© ë‚´ì  | CNN íŠ¹ì§• ì¶”ì¶œ |
| **SVD** | $X = U\Sigma V^\top$ | ì••ì¶•, PCA, LoRA |
| **Whitening** | íƒ€ì› â†’ ì› | VAE, Diffusion ì „ì²˜ë¦¬ |
| **Attention** | ê°€ì¤‘ í‰ê·  | Transformer |

---

## ì°¸ê³  ìë£Œ

- **ì±… ì›ë³¸**: MathGenAIBook12_14_25.tex (Chapter 1)
- **KAIST Challenge í’€ì´**: [KAIST-challenges-solutions.md](KAIST-challenges-solutions.md)
- **Illustrated Transformer**: https://jalammar.github.io/illustrated-transformer/
- **ì›ë…¼ë¬¸**: Vaswani et al., "Attention is All You Need" (2017)
