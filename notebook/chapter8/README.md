# Chapter 8: Energy Based (Graphical) Models

> **ì±… í˜ì´ì§€**: 235-264
> **í•µì‹¬ ì£¼ì œ**: ì—ë„ˆì§€ í•¨ìˆ˜, ê·¸ë˜í”½ ëª¨ë¸, Bayesian Networks, Variational Inference, VAE, RBM, GNN
> **KAIST Challenge ì—°ê²°**: Challenge 16 (Variational Inference), Challenge 17 (VAE), Challenge 18 (Graph Neural Networks)

---

## ğŸ“š ëª©ì°¨

1. [ì—ë„ˆì§€ ê¸°ë°˜ ëª¨ë¸ì´ë€?](#1-ì—ë„ˆì§€-ê¸°ë°˜-ëª¨ë¸ì´ë€)
2. [Graphical Modelsì˜ ë¶„ë¥˜](#2-graphical-modelsì˜-ë¶„ë¥˜)
3. [Bayesian Networks](#3-bayesian-networks)
4. [Variational Inferenceì™€ ELBO](#4-variational-inferenceì™€-elbo)
5. [Mean-Field Approximation](#5-mean-field-approximation)
6. [Variational Auto-Encoder (VAE)](#6-variational-auto-encoder-vae)
7. [Graph Neural Networks (GNN)](#7-graph-neural-networks-gnn)
8. [Generative AIì—ì„œì˜ ì‘ìš©](#8-generative-aiì—ì„œì˜-ì‘ìš©)

---

## 1. ì—ë„ˆì§€ ê¸°ë°˜ ëª¨ë¸ì´ë€?

### ì—ë„ˆì§€ì™€ í™•ë¥ ì˜ ì—°ê²°

> **ì±… ì›ë¬¸ (p.235):**
> "In AI, energy is used metaphorically to define a scalar function over configurations... where lower energy corresponds to higher probability."

$$p(x) = \frac{1}{Z} \exp(-E(x))$$

```
E(x): ì—ë„ˆì§€ í•¨ìˆ˜
    ë‚®ì€ ì—ë„ˆì§€ = ë†’ì€ í™•ë¥  (ì„ í˜¸ë˜ëŠ” ìƒíƒœ)
    ë†’ì€ ì—ë„ˆì§€ = ë‚®ì€ í™•ë¥  (ë¹„ì„ í˜¸ ìƒíƒœ)

Z: ë¶„í•  í•¨ìˆ˜ (Partition Function)
    Z = Î£â‚“ exp(-E(x))
    ëª¨ë“  ìƒíƒœì˜ í•© â†’ ê³„ì‚°ì´ ì–´ë ¤ì›€!
```

### ë¬¼ë¦¬í•™ì—ì„œ AIë¡œ

| ë¬¼ë¦¬í•™ | AI/ML |
|--------|-------|
| ì—ë„ˆì§€ | ë¹„ìš© í•¨ìˆ˜, ìŒì˜ ë¡œê·¸ í™•ë¥  |
| ë‚®ì€ ì—ë„ˆì§€ ìƒíƒœ | ë°ì´í„° manifold ìœ„ì˜ ì  |
| ì˜¨ë„ | ìƒ˜í”Œë§ì˜ ë‹¤ì–‘ì„± ì¡°ì ˆ |
| Boltzmann ë¶„í¬ | ëª¨ë¸ ë¶„í¬ |

### ì™œ ì—ë„ˆì§€ ê¸°ë°˜ì¸ê°€?

```
ì¥ì :
1. ì •ê·œí™” ìƒìˆ˜ ì—†ì´ ì—ë„ˆì§€ ì •ì˜ ê°€ëŠ¥
   E(x)ë§Œ ëª¨ë¸ë§í•˜ë©´ ë¨ (Z ê³„ì‚° ë¶ˆí•„ìš”)

2. ê·¸ë˜í”„ë¡œ ë¶„í•´ ê°€ëŠ¥
   E(x) = Î£áµ¢ Eáµ¢(xáµ¢) + Î£áµ¢â±¼ Eáµ¢â±¼(xáµ¢, xâ±¼)

ë‹¨ì :
ìƒ˜í”Œë§ì´ ì–´ë ¤ì›€ (MCMC í•„ìš”)
```

---

## 2. Graphical Modelsì˜ ë¶„ë¥˜

### ê·¸ë˜í”„ë¡œ í‘œí˜„í•˜ëŠ” í™•ë¥  ëª¨ë¸

> **ì±… ì›ë¬¸ (p.237):**
> "Several classes of graphical models have become central in AI research."

```
ë…¸ë“œ: í™•ë¥  ë³€ìˆ˜
ì—£ì§€: ë³€ìˆ˜ ê°„ ì˜ì¡´ì„±

ê·¸ë˜í”„ êµ¬ì¡° â†’ ë¶„í¬ ë¶„í•´ ê·œì¹™ ê²°ì •
```

### ì£¼ìš” Graphical Models

| ëª¨ë¸ | ê·¸ë˜í”„ íƒ€ì… | íŠ¹ì§• |
|------|-----------|------|
| **Bayesian Networks** | ë°©í–¥ ë¹„ìˆœí™˜ (DAG) | ì¸ê³¼ ê´€ê³„ ëª¨ë¸ë§ |
| **Markov Random Fields** | ë¬´ë°©í–¥ | ì§€ì—­ ì˜ì¡´ì„± |
| **Factor Graphs** | ì´ë¶„ ê·¸ë˜í”„ | ëª…ì‹œì  ë¶„í•´ |
| **Hidden Markov Model** | ì²´ì¸ êµ¬ì¡° | ì‹œê³„ì—´ |

### Ising Model: ê°€ì¥ ê°„ë‹¨í•œ ì˜ˆ

```
ë³€ìˆ˜: x = (xâ‚, ..., xâ‚™), xáµ¢ âˆˆ {-1, +1}

ì—ë„ˆì§€: E(x) = -Î£áµ¢â±¼ Jáµ¢â±¼ xáµ¢ xâ±¼ - Î£áµ¢ háµ¢ xáµ¢

Jáµ¢â±¼: ì´ì›ƒ ìŠ¤í•€ ê°„ ìƒí˜¸ì‘ìš©
háµ¢: ì™¸ë¶€ ìê¸°ì¥

ë†’ì€ ì˜¨ë„: ìŠ¤í•€ ë¬´ì§ˆì„œ
ë‚®ì€ ì˜¨ë„: ì •ë ¬ëœ ìƒíƒœ (ìƒì „ì´)
```

---

## 3. Bayesian Networks

### êµ¬ì¡°

> **ì±… ì›ë¬¸ (p.238):**
> "In a Bayesian network, the joint distribution factorizes according to the network structure."

$$P(X_1, ..., X_n) = \prod_{i=1}^n P(X_i | \text{Parents}(X_i))$$

```
ì˜ˆ: A â†’ B, A â†’ C

P(A, B, C) = P(A) Ã— P(B|A) Ã— P(C|A)

Aê°€ Bì™€ Cì˜ ê³µí†µ ì›ì¸
```

### ì˜ˆì‹œ: ì˜ë£Œ ì§„ë‹¨ (ì±… Example 8.1.1)

```
A: ì§ˆë³‘ ìœ ë¬´ (0 or 1)
B: ì¦ìƒ 1 (Aì— ì˜ì¡´)
C: ì¦ìƒ 2 (Aì— ì˜ì¡´)

P(A=1) = 0.3
P(B=1|A=1) = 0.8,  P(B=1|A=0) = 0.2
P(C=1|A=1) = 0.7,  P(C=1|A=0) = 0.4

P(A=1, B=1, C=1) = 0.3 Ã— 0.8 Ã— 0.7 = 0.168
```

### Hidden Markov Model (HMM)

```
ìˆ¨ê²¨ì§„ ìƒíƒœ: Sâ‚ â†’ Sâ‚‚ â†’ Sâ‚ƒ â†’ ...
         â†“     â†“     â†“
ê´€ì¸¡:      Oâ‚    Oâ‚‚    Oâ‚ƒ

P(S, O) = P(Sâ‚) P(Oâ‚|Sâ‚) Ã— Î â‚œ P(Sâ‚œ|Sâ‚œâ‚‹â‚) P(Oâ‚œ|Sâ‚œ)

ì‘ìš©: ìŒì„± ì¸ì‹, ì‹œê³„ì—´ ë¶„ì„
```

---

## 4. Variational Inferenceì™€ ELBO

### ë¬¸ì œ: ì‚¬í›„ ë¶„í¬ ê³„ì‚°

$$p(z|x) = \frac{p(x|z) p(z)}{p(x)} = \frac{p(x|z) p(z)}{\int p(x|z) p(z) dz}$$

"ë¶„ëª¨ì˜ ì ë¶„ì´ intractable!"

### í•´ê²°: ë³€ë¶„ ê·¼ì‚¬

> **ì±… ì›ë¬¸ (p.241):**
> "Variational inference posits a surrogate distribution q(x|Î¸) and seeks to find parameters Î¸ that minimize the KL divergence."

$$\min_\theta D_{KL}(q(x|\theta) \| p(x))$$

### ELBO (Evidence Lower Bound)

$$\log p(x) \geq \mathbb{E}_{q(z|x)}[\log p(x|z)] - D_{KL}(q(z|x) \| p(z))$$

```
ELBO = E[log p(x|z)] - KL(q(z|x) || p(z))

ì²« í•­: ì¬êµ¬ì„± í’ˆì§ˆ
ë‘˜ì§¸ í•­: ì •ê·œí™”

ELBO ìµœëŒ€í™” â‰ˆ KL(q||p) ìµœì†Œí™”
```

### ELBO ì¦ëª… ìŠ¤ì¼€ì¹˜

```
Jensen's Inequality:
    log E[X] â‰¥ E[log X]

ì ìš©:
    log p(x) = log Î£_z p(x,z)
             = log Î£_z q(z) Ã— p(x,z)/q(z)
             = log E_q[p(x,z)/q(z)]
             â‰¥ E_q[log(p(x,z)/q(z))]
             = E_q[log p(x|z)] + E_q[log p(z)/q(z)]
             = E_q[log p(x|z)] - KL(q||p)
```

---

## 5. Mean-Field Approximation

### ì•„ì´ë””ì–´

> **ì±… ì›ë¬¸ (p.242):**
> "Under mean-field, the surrogate distribution takes a fully factorized form."

$$q(x) = \prod_i q_i(x_i)$$

"ê° ë³€ìˆ˜ê°€ ë…ë¦½ì´ë¼ê³  ê°€ì •" (ì‹¤ì œë¡œëŠ” ìƒê´€ ìˆì§€ë§Œ!)

### Ising Modelì—ì„œì˜ Mean-Field

```
ì‹¤ì œ: p(x) âˆ exp(Î£áµ¢â±¼ Jáµ¢â±¼ xáµ¢xâ±¼ + Î£áµ¢ háµ¢xáµ¢)
      ìŠ¤í•€ë“¤ì´ ì„œë¡œ ì˜í–¥

Mean-Field: q(x) = Î áµ¢ q(xáµ¢)
            ê° ìŠ¤í•€ ë…ë¦½

ìê¸° ì¼ê´€ì„± ë°©ì •ì‹:
    máµ¢ = tanh(háµ¢ + Î£â±¼ Jáµ¢â±¼ mâ±¼)

máµ¢ = E_q[xáµ¢] = "í‰ê·  ìí™”"
```

### Mean-Fieldì˜ í•œê³„

```
ì¥ì :
- ê³„ì‚° íš¨ìœ¨ì 
- ë‹«íŒ í˜•íƒœ ì—…ë°ì´íŠ¸

ë‹¨ì :
- ìƒê´€ê´€ê³„ ë¬´ì‹œ
- ë¶„ì‚° ê³¼ì†Œì¶”ì •
- ë‹¤ë´‰ ë¶„í¬ ì˜ ëª» ì¡ìŒ
```

### Belief Propagation (BP)

```
Tree-structured graphì—ì„œ:
    BPê°€ ì •í™•í•œ ì¶”ë¡ !

Loopê°€ ìˆìœ¼ë©´:
    Loopy BP â†’ ê·¼ì‚¬

Bethe Approximation:
    q(x) = Î _edge q(xáµ¢, xâ±¼) / Î _node q(xáµ¢)^(degree-1)
```

---

## 6. Variational Auto-Encoder (VAE)

### êµ¬ì¡°

> **ì±… ì›ë¬¸ (p.247):**
> "VAEs merges the ideas of variational inference from Bayesian statistics with deep neural network architectures."

```
ì¸ì½”ë” (Encoder): q_Ï†(z|x)
    x â†’ Neural Net â†’ (Î¼, Ïƒ)
    z ~ N(Î¼, ÏƒÂ²)

ë””ì½”ë” (Decoder): p_Î¸(x|z)
    z â†’ Neural Net â†’ xÌ‚

Prior: p(z) = N(0, I)
```

### VAEì˜ ELBO

$$\mathcal{L}(\theta, \phi; x) = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) \| p(z))$$

```
ì²« í•­ (Reconstruction):
    zì—ì„œ xë¥¼ ì˜ ë³µì›í•˜ë‚˜?
    Binary: BCE loss
    Continuous: MSE loss

ë‘˜ì§¸ í•­ (KL Regularization):
    ì¸ì½”ë” ì¶œë ¥ì´ N(0,I)ì— ê°€ê¹Œìš´ê°€?

    KL(N(Î¼,ÏƒÂ²) || N(0,1))
    = (1/2) Î£ (Î¼Â² + ÏƒÂ² - log ÏƒÂ² - 1)
```

### Reparameterization Trick

```
ë¬¸ì œ: z ~ q(z|x)ì—ì„œ Î¸ë¡œ gradient ëª» í˜ë¦¼

í•´ê²°:
    Îµ ~ N(0, 1)
    z = Î¼ + Ïƒ Ã— Îµ

    zê°€ ì´ì œ Î¼, Ïƒì˜ í•¨ìˆ˜!
    â†’ backprop ê°€ëŠ¥
```

### ìƒì„± ê³¼ì •

```
í•™ìŠµ í›„:
    1. z ~ N(0, I) ìƒ˜í”Œë§
    2. x = Decoder(z)
    â†’ ìƒˆë¡œìš´ ë°ì´í„° ìƒì„±!
```

---

## 7. Graph Neural Networks (GNN)

### ì™œ ê·¸ë˜í”„ì— Neural Network?

```
ì¼ë°˜ NN: ê³ ì •ëœ í¬ê¸° ì…ë ¥ (ì´ë¯¸ì§€, ë²¡í„°)
GNN: ì„ì˜ì˜ ê·¸ë˜í”„ êµ¬ì¡° ì…ë ¥

ì‘ìš©:
- ë¶„ì êµ¬ì¡° ì˜ˆì¸¡
- ì†Œì…œ ë„¤íŠ¸ì›Œí¬
- ì¶”ì²œ ì‹œìŠ¤í…œ
- ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜
```

### Message Passing Framework

$$h_i^{(l+1)} = \text{UPDATE}\left(h_i^{(l)}, \text{AGGREGATE}(\{h_j^{(l)} : j \in \mathcal{N}(i)\})\right)$$

```
1. AGGREGATE: ì´ì›ƒ ë…¸ë“œì˜ ì •ë³´ ëª¨ìŒ
   ì˜ˆ: í‰ê· , í•©, max

2. UPDATE: ìì‹ ì˜ í‘œí˜„ ì—…ë°ì´íŠ¸
   ì˜ˆ: MLP, GRU

kë²ˆ ë°˜ë³µ â†’ k-hop ì´ì›ƒ ì •ë³´ í†µí•©
```

### GCN (Graph Convolutional Network)

$$H^{(l+1)} = \sigma(\tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} H^{(l)} W^{(l)})$$

```
Ãƒ = A + I (self-loop ì¶”ê°€)
DÌƒ: degree matrix

"ì´ì›ƒì˜ ê°€ì¤‘ í‰ê·  + ì„ í˜• ë³€í™˜ + í™œì„±í™”"
```

### Spectral vs Spatial

| ë°©ì‹ | ì•„ì´ë””ì–´ | ì¥ë‹¨ì  |
|------|---------|--------|
| **Spectral** | ê·¸ë˜í”„ í‘¸ë¦¬ì— ë³€í™˜ | ì´ë¡ ì , ë¹„íš¨ìœ¨ì  |
| **Spatial** | Message passing | íš¨ìœ¨ì , ì§ê´€ì  |

---

## 8. Generative AIì—ì„œì˜ ì‘ìš©

### VAE: ì ì¬ ê³µê°„ ìƒì„±

```
VAEì˜ ì ì¬ ê³µê°„:
    - ì—°ì†ì , êµ¬ì¡°í™”ë¨
    - ë³´ê°„ ê°€ëŠ¥ (z1ê³¼ z2 ì‚¬ì´)
    - ì¡°ì‘ ê°€ëŠ¥ (íŠ¹ì • ë°©í–¥ = íŠ¹ì • ì†ì„±)

í•œê³„:
    - íë¦¿í•œ ì´ë¯¸ì§€ ê²½í–¥
    - KL collapse ë¬¸ì œ
```

### VAE vs GAN vs Diffusion

| ëª¨ë¸ | í•™ìŠµ ëª©í‘œ | ìƒ˜í”Œë§ | í’ˆì§ˆ |
|------|----------|--------|------|
| **VAE** | ELBO ìµœëŒ€í™” | z â†’ Decoder | íë¦¿í•¨ |
| **GAN** | Adversarial | z â†’ Generator | ì„ ëª…, ë¶ˆì•ˆì • |
| **Diffusion** | Score matching | ë°˜ë³µ denoising | SOTA |

### Energy-Based Models (EBMs)

```
E_Î¸(x): í•™ìŠµëœ ì—ë„ˆì§€ í•¨ìˆ˜

p(x) âˆ exp(-E_Î¸(x))

í•™ìŠµ: Contrastive Divergence
ìƒ˜í”Œë§: Langevin / MCMC

ì¥ì : ìœ ì—°í•œ ëª¨ë¸ë§
ë‹¨ì : ëŠë¦° ìƒ˜í”Œë§
```

### GNN for Molecular Generation

```
ë¶„ì = ê·¸ë˜í”„
    ë…¸ë“œ: ì›ì
    ì—£ì§€: ê²°í•©

GNN ìƒì„±:
    1. ë…¸ë“œ ì„ë² ë”© í•™ìŠµ
    2. Auto-regressiveë¡œ ë…¸ë“œ/ì—£ì§€ ì¶”ê°€
    ë˜ëŠ”
    VAEì˜ latent space â†’ ë¶„ì

ì‘ìš©: ì‹ ì•½ ì„¤ê³„, ì¬ë£Œ ë°œê²¬
```

### Score-Based Models (Ch.9 ë¯¸ë¦¬ë³´ê¸°)

```
Score: s(x) = âˆ‡_x log p(x)

ì—ë„ˆì§€ì™€ì˜ ê´€ê³„:
    s(x) = -âˆ‡_x E(x)

Diffusion:
    Forward: ì ì  noise ì¶”ê°€
    Reverse: Scoreë¡œ denoise

"Energy-Based + Stochastic Process"
```

---

## ğŸ“ í•µì‹¬ ì •ë¦¬

### ì´ ì±•í„°ì—ì„œ ê¼­ ê¸°ì–µí•  ê²ƒ

1. **ì—ë„ˆì§€ â†” í™•ë¥ **
   - $p(x) \propto \exp(-E(x))$
   - ë‚®ì€ ì—ë„ˆì§€ = ë†’ì€ í™•ë¥ 

2. **Graphical Models**
   - ê·¸ë˜í”„ = ì¡°ê±´ë¶€ ë…ë¦½ êµ¬ì¡°
   - ë¶„í¬ì˜ íš¨ìœ¨ì  ë¶„í•´

3. **Variational Inference**
   - ì–´ë ¤ìš´ ë¶„í¬ë¥¼ ì‰¬ìš´ ë¶„í¬ë¡œ ê·¼ì‚¬
   - ELBO ìµœëŒ€í™”

4. **VAE = Variational + Neural Network**
   - Encoder: q(z|x)
   - Decoder: p(x|z)
   - Reparameterization trick

5. **GNN = ê·¸ë˜í”„ ìœ„ì˜ Neural Network**
   - Message passing
   - êµ¬ì¡°ì  ë°ì´í„° ì²˜ë¦¬

---

## ğŸ”— ë‹¤ë¥¸ ì±•í„°ì™€ì˜ ì—°ê²°

| ì—°ê²° | ì„¤ëª… |
|------|------|
| **Ch.5 â†’ Ch.8** | í™•ë¥  ë¶„í¬ â†’ ì—ë„ˆì§€ í•´ì„ |
| **Ch.6 â†’ Ch.8** | KL â†’ ELBO, VAE |
| **Ch.7 â†’ Ch.8** | MCMC â†’ EBM ìƒ˜í”Œë§ |
| **Ch.4 â†’ Ch.8** | Neural Net â†’ VAE, GNN |
| **Ch.8 â†’ Ch.9** | EBM/VAE â†’ Diffusion í†µí•© |

---

*ì´ ë¬¸ì„œëŠ” Mathematics of Generative AI Book Chapter 8ì˜ í•™ìŠµ ê°€ì´ë“œì…ë‹ˆë‹¤.*
