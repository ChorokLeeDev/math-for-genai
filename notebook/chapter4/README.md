# Chapter 4: Neural Networks & Deep Learning

> **ì±… í˜ì´ì§€**: 99-132
> **í•µì‹¬ ì£¼ì œ**: ì‹ ê²½ë§ ë©”ì»¤ë‹ˆì¦˜, CNN, ResNet, Neural ODE, í‘œí˜„ í•™ìŠµ
> **KAIST Challenge ì—°ê²°**: Challenge 7 (ResNet & Skip Connections), Challenge 8 (Neural ODE)

---

## ğŸ“š ëª©ì°¨

1. [ì‹ ê²½ë§ì´ ë­”ê°€ìš”?](#1-ì‹ ê²½ë§ì´-ë­”ê°€ìš”)
2. [Neural Network Mechanics](#2-neural-network-mechanics)
3. [CNN: ì´ë¯¸ì§€ë¥¼ ìœ„í•œ ì‹ ê²½ë§](#3-cnn-ì´ë¯¸ì§€ë¥¼-ìœ„í•œ-ì‹ ê²½ë§)
4. [ResNet: Skip Connectionì˜ í˜](#4-resnet-skip-connectionì˜-í˜)
5. [Neural ODE: ì—°ì†ì ì¸ ê¹Šì´](#5-neural-ode-ì—°ì†ì ì¸-ê¹Šì´)
6. [Notebooks ê°€ì´ë“œ](#6-notebooks-ê°€ì´ë“œ)
7. [Generative AIì—ì„œì˜ ì‘ìš©](#7-generative-aiì—ì„œì˜-ì‘ìš©)

---

## 1. ì‹ ê²½ë§ì´ ë­”ê°€ìš”?

### ì• ì±•í„°ë“¤ê³¼ì˜ ì—°ê²°

> **ì±… ì›ë¬¸ (p.99):**
> "We begin this chapter by situating neural networks within the mathematical framework developed in Chapters 1,2,3."

ì‹ ê²½ë§ì€ ì•ì„œ ë°°ìš´ ëª¨ë“  ê²ƒì˜ **ì§‘í•©ì²´**ì…ë‹ˆë‹¤:

| ì±•í„° | ê°œë… | ì‹ ê²½ë§ì—ì„œì˜ ì—­í•  |
|------|------|------------------|
| **Ch.1** | í–‰ë ¬, í…ì„œ, ì„ í˜• ë³€í™˜ | ê° ì¸µì˜ ê°€ì¤‘ì¹˜, ì…ë ¥ ë°ì´í„° í‘œí˜„ |
| **Ch.2** | ODE, ë™ì  ì‹œìŠ¤í…œ | ResNet = ODEì˜ ì´ì‚°í™” |
| **Ch.3** | ìµœì í™”, GD | ì†ì‹¤í•¨ìˆ˜ ìµœì†Œí™”ë¡œ í•™ìŠµ |

### ì‹ ê²½ë§ì˜ ë³¸ì§ˆ

```
ì‹ ê²½ë§ = "ì¸µì„ ìŒ“ì•„ì„œ ë³µì¡í•œ í•¨ìˆ˜ë¥¼ ë§Œë“œëŠ” ê²ƒ"

ì…ë ¥ x â†’ [ì¸µ1] â†’ [ì¸µ2] â†’ ... â†’ [ì¸µL] â†’ ì¶œë ¥ Å·

ê° ì¸µ:  h_{k+1} = Ïƒ(W_k Â· h_k + b_k)

ì—¬ê¸°ì„œ:
- W_k: ê°€ì¤‘ì¹˜ í–‰ë ¬ (í•™ìŠµë¨)
- b_k: í¸í–¥ ë²¡í„° (í•™ìŠµë¨)
- Ïƒ: í™œì„±í™” í•¨ìˆ˜ (ReLU, tanh ë“±)
```

### ì™œ "Deep" Learningì¸ê°€?

```
ì–•ì€ ì‹ ê²½ë§:  ì…ë ¥ â†’ [ì¸µ1] â†’ ì¶œë ¥
             ê°„ë‹¨í•œ íŒ¨í„´ë§Œ í•™ìŠµ ê°€ëŠ¥

ê¹Šì€ ì‹ ê²½ë§:  ì…ë ¥ â†’ [ì¸µ1] â†’ [ì¸µ2] â†’ ... â†’ [ì¸µ100] â†’ ì¶œë ¥
             ê³„ì¸µì ìœ¼ë¡œ ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ

             ì¸µ1: ì„  ê²€ì¶œ
             ì¸µ2: ëª¨ì„œë¦¬, ì§ˆê°
             ì¸µ3: ë¶€ë¶„ (ëˆˆ, ì½”)
             ì¸µL: ì „ì²´ ê°œë… (ì–¼êµ´)
```

---

## 2. Neural Network Mechanics

### Perceptron: ì‹œì‘ì  (1958)

> **ì±… ì›ë¬¸ (p.101):**
> "A perceptron takes an input vector x âˆˆ â„^d, computes a linear score wâŠ¤x + b, and then applies a threshold nonlinearity."

```
Å· = sign(wâŠ¤x + b)

ë¬¸ì œ: ì„ í˜• ë¶„ë¦¬ ê°€ëŠ¥í•œ ë¬¸ì œë§Œ í’€ ìˆ˜ ìˆìŒ!
      XOR ë¬¸ì œì¡°ì°¨ ëª» í’ˆ
```

**Minsky & Papert (1969)**: "ë‹¨ì¸µ í¼ì…‰íŠ¸ë¡ ì€ XORì„ ëª» í‘¼ë‹¤!"
â†’ ì´ê²Œ **ë‹¤ì¸µ ì‹ ê²½ë§**ì˜ í•„ìš”ì„±ì„ ë¶€ê°ì‹œí‚´

### ë‹¨ì¼ ì€ë‹‰ì¸µ ì‹ ê²½ë§

```
ì…ë ¥ (xâ‚, xâ‚‚)
    â†“
ì€ë‹‰ì¸µ: h = tanh(Wâ‚x + bâ‚)   â† 10ê°œ ë‰´ëŸ°
    â†“
ì¶œë ¥ì¸µ: Å· = Ïƒ(Wâ‚‚h + bâ‚‚)      â† í™•ë¥  ì¶œë ¥
```

**í•µì‹¬ ì°¨ì´: íŠ¹ì§• í•™ìŠµ (Feature Learning)**

| ë°©ë²• | íŠ¹ì§• | ì¥ë‹¨ì  |
|------|------|--------|
| **ë¡œì§€ìŠ¤í‹± íšŒê·€ + ë‹¤í•­ì‹** | ì†ìœ¼ë¡œ ì„¤ê³„: $\phi(x) = [x_1, x_2, x_1^2, x_1x_2, ...]$ | ë„ë©”ì¸ ì§€ì‹ í•„ìš” |
| **ì‹ ê²½ë§** | ìë™ í•™ìŠµ: $h = \sigma(Wx + b)$ | ë°ì´í„°ì—ì„œ íŠ¹ì§• ë°œê²¬ |

> **ì±… ì›ë¬¸ (p.104):**
> "A neural network introduces a learned nonlinear feature map through its hidden layers... Feature engineering is replaced by representation learning."

### Interpolation vs Extrapolation

**í¥ë¯¸ë¡œìš´ ê´€ì°°** (ì±… Example 4.1.2):

```
ë°ì´í„°: 5ê°œ ì , í›ˆë ¨ êµ¬ê°„ [-1, 1]

ë‹¤í•­ì‹ íšŒê·€ (ì°¨ìˆ˜ 9):
- Interpolation: í›ˆë ¨ ì ì€ ì™„ë²½íˆ í†µê³¼
- í•˜ì§€ë§Œ: ì§„ë™ì´ ì‹¬í•¨ (Runge í˜„ìƒ)
- Extrapolation: ì™„ì „íˆ ë°œì‚°

ì‹ ê²½ë§ (50ê°œ ì€ë‹‰ ë‰´ëŸ°):
- Interpolation: ë¶€ë“œëŸ¬ìš´ ê³¡ì„ 
- íŒŒë¼ë¯¸í„° >> ë°ì´í„°ì¸ë°ë„ ê³¼ì í•© ì—†ìŒ!
- Extrapolation: ì—­ì‹œ ì‹¤íŒ¨ (ì™¸ì‚½ì€ ì–´ë ¤ì›€)
```

**í•µì‹¬ êµí›ˆ**:
- ì‹ ê²½ë§ì€ **interpolation**ì—ì„œ ë†€ëë„ë¡ ì•ˆì •ì 
- í•˜ì§€ë§Œ **extrapolation**ì€ ì–´ë–¤ ëª¨ë¸ë„ ì–´ë µë‹¤
- Over-parameterizationì´ ë°˜ë“œì‹œ ê³¼ì í•©ì„ ì˜ë¯¸í•˜ì§€ ì•ŠìŒ

---

## 3. CNN: ì´ë¯¸ì§€ë¥¼ ìœ„í•œ ì‹ ê²½ë§

### ì™œ Fully Connectedê°€ ì´ë¯¸ì§€ì— ë¶€ì í•©í•œê°€?

```
28Ã—28 MNIST ì´ë¯¸ì§€ë¥¼ FCë¡œ ì²˜ë¦¬í•˜ë©´?
ì…ë ¥: 784ì°¨ì›
ì²« ì¸µ (100 ë‰´ëŸ°): 784 Ã— 100 = 78,400 íŒŒë¼ë¯¸í„°

224Ã—224 ì»¬ëŸ¬ ì´ë¯¸ì§€ë¼ë©´?
ì…ë ¥: 224 Ã— 224 Ã— 3 = 150,528ì°¨ì›
ì²« ì¸µ: 1.5ì–µ íŒŒë¼ë¯¸í„°?! â†’ ë¶ˆê°€ëŠ¥
```

### CNNì˜ í•µì‹¬ ì•„ì´ë””ì–´

> **ì±… ì›ë¬¸ (p.107):**
> "Convolutional Neural Networks exploit exactly this kind of prior structure: locality and weight sharing."

| ì›ë¦¬ | ì„¤ëª… | íš¨ê³¼ |
|------|------|------|
| **êµ­ì†Œì„± (Locality)** | ê° ë‰´ëŸ°ì€ ì‘ì€ ì˜ì—­ë§Œ ë´„ | íŒŒë¼ë¯¸í„° ëŒ€í­ ê°ì†Œ |
| **ê°€ì¤‘ì¹˜ ê³µìœ  (Weight Sharing)** | ê°™ì€ í•„í„°ë¥¼ ì „ì²´ ì´ë¯¸ì§€ì— ì ìš© | ìœ„ì¹˜ ë¶ˆë³€ì„± |
| **ê³„ì¸µì  íŠ¹ì§•** | ì–•ì€ ì¸µ: ì—£ì§€ â†’ ê¹Šì€ ì¸µ: ê°ì²´ | ì¶”ìƒí™” í•™ìŠµ |

### CNN êµ¬ì¡° ì˜ˆì‹œ (MNIST)

```
ì…ë ¥: 28Ã—28Ã—1
    â†“
Conv1: 16 í•„í„°, 3Ã—3, padding=1 â†’ 28Ã—28Ã—16
MaxPool: 2Ã—2 â†’ 14Ã—14Ã—16
    â†“
Conv2: 32 í•„í„°, 3Ã—3, padding=1 â†’ 14Ã—14Ã—32
MaxPool: 2Ã—2 â†’ 7Ã—7Ã—32
    â†“
Flatten: 1568ì°¨ì›
FC1: 128 ë‰´ëŸ°
FC2: 10 ë‰´ëŸ° (í´ë˜ìŠ¤)
    â†“
Softmax â†’ í™•ë¥  ë¶„í¬
```

### ì»¨ë³¼ë£¨ì…˜ ì—°ì‚° ë³µìŠµ (Ch.1 ì—°ê²°)

$$y_{i,j} = \sum_{h,w} k_{h,w} \cdot x_{i+h, j+w}$$

```
ì…ë ¥ ì´ë¯¸ì§€:          ì»¤ë„ (í•„í„°):
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”    â”Œâ”€â”€â”€â”¬â”€â”€â”€â”
â”‚ 1 â”‚ 2 â”‚ 3 â”‚ 0 â”‚    â”‚ 1 â”‚ 0 â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤    â”œâ”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚ 4 â”‚ 5 â”‚ 6 â”‚ 1 â”‚    â”‚ 0 â”‚-1 â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤    â””â”€â”€â”€â”´â”€â”€â”€â”˜
â”‚ 7 â”‚ 8 â”‚ 9 â”‚ 0 â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚ 1 â”‚ 0 â”‚ 2 â”‚ 3 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜

ì¶œë ¥ (0,0) = 1Ã—1 + 2Ã—0 + 4Ã—0 + 5Ã—(-1) = -4
```

### ì±„ë„ ìˆ˜ ì„ íƒ ì›ì¹™

> **ì±… ì›ë¬¸ (p.110):**

1. **ì–•ì€ ì¸µ**: ê°„ë‹¨í•œ íŠ¹ì§• (ì—£ì§€, ì½”ë„ˆ) â†’ 16-64 ì±„ë„
2. **ê¹Šì€ ì¸µ**: ë³µì¡í•œ íŠ¹ì§• (ê°ì²´ ë¶€ë¶„) â†’ 128-256+ ì±„ë„
3. **ê²½í—˜ì  ê·œì¹™**: í•´ìƒë„ê°€ ë°˜ìœ¼ë¡œ ì¤„ë©´, ì±„ë„ì€ ë‘ ë°°ë¡œ

---

## 4. ResNet: Skip Connectionì˜ í˜

### ë¬¸ì œ: ê¹Šì€ ì‹ ê²½ë§ì˜ í•™ìŠµ ì–´ë ¤ì›€

```
ì¸µì„ ê¹Šê²Œ ìŒ“ìœ¼ë©´ ì¢‹ì„ê¹Œ?

20ì¸µ ì‹ ê²½ë§:  ì •í™•ë„ 92%
56ì¸µ ì‹ ê²½ë§:  ì •í™•ë„ 89%  â† ì˜¤íˆë ¤ ë‚˜ë¹ ì§!

ì™œ? Gradient Vanishing/Exploding
ê¹Šì€ ì¸µì˜ gradientê°€ ì•ìª½ê¹Œì§€ ì „ë‹¬ì´ ì•ˆ ë¨
```

### ResNetì˜ í•´ê²°ì±…: Residual Block

> **ì±… ì›ë¬¸ (p.113):**
> "The key idea is to make each layer learn a residual correction on top of an identity map."

```
ê¸°ì¡´ ë°©ì‹:
    h_{t+1} = F(h_t)
    "h_të¥¼ ì™„ì „íˆ ìƒˆë¡œìš´ h_{t+1}ë¡œ ë³€í™˜"

ResNet ë°©ì‹:
    h_{t+1} = h_t + F(h_t)
    "h_tì— ì‘ì€ ë³€í™”ëŸ‰ F(h_t)ë§Œ ë”í•¨"
         â†‘
    Skip Connection (Identity Mapping)
```

**ì™œ ì´ê²Œ íš¨ê³¼ì ì¸ê°€?**

```
Jacobian ê´€ì :

ê¸°ì¡´: âˆ‚h_{t+1}/âˆ‚h_t = âˆ‚F/âˆ‚h_t
      â†’ í–‰ë ¬ ê³±ì´ ëˆ„ì ë˜ë©´ í­ë°œ/ì†Œë©¸

ResNet: âˆ‚h_{t+1}/âˆ‚h_t = I + âˆ‚F/âˆ‚h_t
        â†’ Identity Iê°€ gradient ì „ë‹¬ì„ ë³´ì¥!
```

### ResNet = ODEì˜ ì´ì‚°í™”!

**í•µì‹¬ í†µì°°** (ì±… p.114):

$$h_{t+1} = h_t + F(h_t, \theta_t)$$

ì´ê±´ ODEì˜ **Euler method**:

$$x(t+\Delta t) = x(t) + f(x(t)) \cdot \Delta t$$

| ResNet | ODE |
|--------|-----|
| ì¸µ ì¸ë±ìŠ¤ $t$ | ì‹œê°„ $t$ |
| $h_t$ (ì¸µ ì¶œë ¥) | $x(t)$ (ìƒíƒœ) |
| $F(h_t)$ | $f(x(t)) \cdot \Delta t$ |
| ì¸µ í†µê³¼ | ì‹œê°„ 1 ìŠ¤í… |

---

## 5. Neural ODE: ì—°ì†ì ì¸ ê¹Šì´

### ResNetì—ì„œ Neural ODEë¡œ

> **ì±… ì›ë¬¸ (p.114):**
> "If we imagine L large and the changes per layer small, this composition suggests a continuous-depth limit."

```
ResNet (ì´ì‚°ì ):
    h_0 â†’ h_1 â†’ h_2 â†’ ... â†’ h_L
    "Lê°œì˜ ì¸µì„ ì°¨ë¡€ë¡œ í†µê³¼"

Neural ODE (ì—°ì†ì ):
    dh(t)/dt = f(h(t), t; Î¸)
    h(0) â†’ âˆ« â†’ h(T)
    "ì—°ì†ì ì¸ ë³€í™˜"
```

### Neural ODEì˜ ì¥ì 

| í•­ëª© | ResNet | Neural ODE |
|------|--------|------------|
| **ë©”ëª¨ë¦¬** | ëª¨ë“  ì¤‘ê°„ $h_t$ ì €ì¥ | $h(0)$, $h(T)$ë§Œ ì €ì¥ |
| **ê¹Šì´** | ê³ ì • (ì˜ˆ: 50ì¸µ) | ì ë¶„ ì‹œê°„ $T$ ì¡°ì ˆ |
| **Gradient** | Backprop through layers | Adjoint method |
| **í•´ì„** | ì´ì‚°ì  ë³€í™˜ | ì—°ì†ì  íë¦„ |

### Adjoint Method: ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í•™ìŠµ

ë¬¸ì œ: $h(T)$ì—ì„œ $\theta$ë¡œ ì–´ë–»ê²Œ gradientë¥¼?

**ì¼ë°˜ì  Backprop**: ëª¨ë“  ì¤‘ê°„ ìƒíƒœ ì €ì¥ â†’ ë©”ëª¨ë¦¬ $O(L)$

**Adjoint Method**:
1. Forward: $h(0) \to h(T)$ ê³„ì‚°
2. Backward: ë³„ë„ì˜ ODEë¡œ gradient ê³„ì‚°

$$\frac{da(t)}{dt} = -a(t)^\top \frac{\partial f}{\partial h}$$

ë©”ëª¨ë¦¬: $O(1)$ (ìƒìˆ˜!)

### ì‹¤ìŠµ: Spiral ë°ì´í„°ì…‹

ë…¸íŠ¸ë¶ `NeuralODE-Spiral.ipynb`ì—ì„œ:
- 2D spiral ë°ì´í„° ë¶„ë¥˜
- Neural ODEê°€ ì—°ì†ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ëŠ” ê³¼ì • ì‹œê°í™”
- ResNetê³¼ ë¹„êµ

---

## 6. Notebooks ê°€ì´ë“œ

### NN/ í´ë”

| ë…¸íŠ¸ë¶ | ë­˜ ë°°ìš°ë‚˜? | í•µì‹¬ ì‹¤ìŠµ |
|--------|-----------|----------|
| `LogReg+NN-supervised-2D.ipynb` | NN vs ë¡œì§€ìŠ¤í‹± íšŒê·€ | 2D ë¶„ë¥˜ ë¹„êµ |
| `InterPoly-vs-NN.ipynb` | ë³´ê°„ vs ì™¸ì‚½ | ë‹¤í•­ì‹ vs NN ë¹„êµ |
| `cnn-simple-MNIST.ipynb` | CNN ê¸°ì´ˆ | MNIST ë¶„ë¥˜ |
| `CNN-MNIST-PCA.ipynb` | CNN + ì°¨ì›ì¶•ì†Œ | íŠ¹ì§• ì‹œê°í™” |
| `ResNet9-Spiral.ipynb` | ResNet êµ¬ì¡° | Skip connection íš¨ê³¼ |
| `NeuralODE-Spiral.ipynb` | Neural ODE | ì—°ì†ì  ë³€í™˜ |
| `NeuralODE-Adjoint-Spiral.ipynb` | Adjoint method | ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í•™ìŠµ |
| `NN-Decoding.ipynb` | ì‹ ê²½ë§ ë””ì½”ë”© | ì •ë³´ ì´ë¡  ì—°ê²° |

### ê¼­ í•´ë³¼ ì‹¤í—˜ë“¤

**1. ì€ë‹‰ì¸µ í¬ê¸° ì‹¤í—˜**
```python
# ì€ë‹‰ ë‰´ëŸ° ìˆ˜: 5, 10, 50, 100
# ì–´ë–»ê²Œ decision boundaryê°€ ë°”ë€Œë‚˜?
```

**2. Skip Connection íš¨ê³¼**
```python
# ResNet9-Spiral.ipynbì—ì„œ
# skip connection ì œê±°í•˜ë©´?
# í•™ìŠµì´ ì–¼ë§ˆë‚˜ ì–´ë ¤ì›Œì§€ë‚˜?
```

**3. Neural ODE vs ResNet**
```python
# ê°™ì€ ë¬¸ì œë¥¼ ë‘ ë°©ë²•ìœ¼ë¡œ í’€ì–´ë³´ê¸°
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, ì •í™•ë„ ë¹„êµ
```

---

## 7. Generative AIì—ì„œì˜ ì‘ìš©

### Diffusion Modelì˜ U-Net

```
Diffusionì˜ í•µì‹¬ ì‹ ê²½ë§ = U-Net

ì¸ì½”ë” (ë‹¤ìš´ìƒ˜í”Œë§):
    64Ã—64 â†’ 32Ã—32 â†’ 16Ã—16 â†’ 8Ã—8

ë””ì½”ë” (ì—…ìƒ˜í”Œë§):
    8Ã—8 â†’ 16Ã—16 â†’ 32Ã—32 â†’ 64Ã—64

Skip Connectionìœ¼ë¡œ ì¸ì½”ë”-ë””ì½”ë” ì—°ê²°!
â†’ ê³ í•´ìƒë„ ë””í…Œì¼ ë³´ì¡´
```

### ResNetì´ Diffusionì—ì„œ ì¤‘ìš”í•œ ì´ìœ 

```
Diffusionì˜ ê° denoising step:
    x_t â†’ [U-Net] â†’ x_{t-1}

U-Net ë‚´ë¶€ì— ResNet Block ì‚¬ìš©:
- ê¹Šì€ ë„¤íŠ¸ì›Œí¬ ì•ˆì •ì  í•™ìŠµ
- Gradient flow ë³´ì¥
- ë…¸ì´ì¦ˆ ì œê±° í’ˆì§ˆ í–¥ìƒ
```

### Neural ODE â†’ Continuous Normalizing Flow

```
Normalizing Flow = í™•ë¥  ë¶„í¬ ë³€í™˜

ì´ì‚°ì : z â†’ f_1 â†’ f_2 â†’ ... â†’ f_K â†’ x

ì—°ì†ì  (CNF):
    dz/dt = f(z, t)

ì¥ì :
- ì„ì˜ì˜ ë³€í™˜ ê°€ëŠ¥ (ì—­ë³€í™˜ ê°€ëŠ¥í•  í•„ìš” ì—†ìŒ)
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
```

### Transformerì˜ Skip Connection

```
Transformer Blockë„ ResNet êµ¬ì¡°!

Attention:
    x = x + Attention(LayerNorm(x))

FFN:
    x = x + FFN(LayerNorm(x))

ìˆ˜ë°± ì¸µë„ ì•ˆì •ì  í•™ìŠµ ê°€ëŠ¥
â†’ GPT, BERT, LLMì˜ ê¸°ë°˜
```

---

## ğŸ“ í•µì‹¬ ì •ë¦¬

### ì´ ì±•í„°ì—ì„œ ê¼­ ê¸°ì–µí•  ê²ƒ

1. **ì‹ ê²½ë§ = Ch.1,2,3ì˜ ì¢…í•©**
   - í–‰ë ¬ ë³€í™˜ + ë™ì  ì‹œìŠ¤í…œ + ìµœì í™”

2. **íŠ¹ì§• í•™ìŠµ (Representation Learning)**
   - ì†ìœ¼ë¡œ ì„¤ê³„ â†’ ë°ì´í„°ì—ì„œ ìë™ í•™ìŠµ

3. **CNN = ì´ë¯¸ì§€ì˜ êµ¬ì¡° í™œìš©**
   - êµ­ì†Œì„± + ê°€ì¤‘ì¹˜ ê³µìœ  + ê³„ì¸µì  íŠ¹ì§•

4. **ResNet = Skip Connection**
   - $h_{t+1} = h_t + F(h_t)$
   - ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì˜ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ í•¨

5. **Neural ODE = ì—°ì†ì  ì‹ ê²½ë§**
   - ResNetì˜ ì—°ì† ë²„ì „
   - ë©”ëª¨ë¦¬ íš¨ìœ¨ì , í•´ì„ ê°€ëŠ¥

---

## ğŸ”— ë‹¤ë¥¸ ì±•í„°ì™€ì˜ ì—°ê²°

| ì—°ê²° | ì„¤ëª… |
|------|------|
| **Ch.1 â†’ Ch.4** | í–‰ë ¬ ì—°ì‚°ì´ ê° ì¸µì˜ ê¸°ë³¸ |
| **Ch.2 â†’ Ch.4** | ResNet = ODEì˜ Euler method |
| **Ch.3 â†’ Ch.4** | SGD/Adamìœ¼ë¡œ ê°€ì¤‘ì¹˜ í•™ìŠµ |
| **Ch.4 â†’ Ch.6** | Autoencoder, U-Net (ì •ë³´ ì••ì¶•) |
| **Ch.4 â†’ Ch.7** | Neural ODE â†’ SDE â†’ Diffusion |
| **Ch.4 â†’ Ch.9** | Diffusionì˜ backbone |

---

*ì´ ë¬¸ì„œëŠ” Mathematics of Generative AI Book Chapter 4ì˜ í•™ìŠµ ê°€ì´ë“œì…ë‹ˆë‹¤.*
